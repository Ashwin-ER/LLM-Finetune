{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyuOCYM92LJb"
      },
      "source": [
        "# Getting Started with Fine-Tuning Mistral 7B\n",
        "\n",
        "This notebook shows you a simple example of how to LoRA finetune Mistral 7B. You can run this notebook in Google Colab with Pro + account with A100 and 40GB RAM.\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/mistralai/mistral-finetune/blob/main/tutorials/mistral_finetune_7b.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "\n",
        "Check out `mistral-finetune` Github repo to learn more: https://github.com/mistralai/mistral-finetune/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxr8mv-17GfB"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Clone the `mistral-finetune` repo:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIj3IlIeVDIb",
        "outputId": "7dbd8142-38c8-42da-ac32-18e68b23cd63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path 'mistral-finetune' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/mistralai/mistral-finetune.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQPd_pGT7WiY"
      },
      "source": [
        "Install all required dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuTOGipl7BS7",
        "outputId": "2d869a7d-f29c-4491-a59a-775380c3fcd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fire in /usr/local/lib/python3.11/dist-packages (from -r /content/mistral-finetune/requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: simple-parsing in /usr/local/lib/python3.11/dist-packages (from -r /content/mistral-finetune/requirements.txt (line 2)) (0.1.7)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from -r /content/mistral-finetune/requirements.txt (line 3)) (6.0.2)\n",
            "Requirement already satisfied: mistral-common>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from -r /content/mistral-finetune/requirements.txt (line 4)) (1.5.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from -r /content/mistral-finetune/requirements.txt (line 5)) (0.5.3)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from -r /content/mistral-finetune/requirements.txt (line 6)) (2.18.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from -r /content/mistral-finetune/requirements.txt (line 7)) (4.67.1)\n",
            "Collecting torch==2.2 (from -r /content/mistral-finetune/requirements.txt (line 9))\n",
            "  Using cached torch-2.2.0-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting triton==2.2 (from -r /content/mistral-finetune/requirements.txt (line 10))\n",
            "  Using cached triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: xformers==0.0.24 in /usr/local/lib/python3.11/dist-packages (from -r /content/mistral-finetune/requirements.txt (line 11)) (0.0.24)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9)) (4.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9)) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9)) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9))\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9))\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9))\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9))\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9))\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9))\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9))\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9))\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9))\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9))\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9))\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xformers==0.0.24->-r /content/mistral-finetune/requirements.txt (line 11)) (1.23.5)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9)) (12.4.127)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->-r /content/mistral-finetune/requirements.txt (line 1)) (3.0.1)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.11/dist-packages (from simple-parsing->-r /content/mistral-finetune/requirements.txt (line 2)) (0.16)\n",
            "Requirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.11/dist-packages (from mistral-common>=1.3.1->-r /content/mistral-finetune/requirements.txt (line 4)) (4.23.0)\n",
            "Collecting numpy (from xformers==0.0.24->-r /content/mistral-finetune/requirements.txt (line 11))\n",
            "  Using cached numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Requirement already satisfied: pillow>=10.3.0 in /usr/local/lib/python3.11/dist-packages (from mistral-common>=1.3.1->-r /content/mistral-finetune/requirements.txt (line 4)) (11.1.0)\n",
            "Requirement already satisfied: pydantic<3.0,>=2.7 in /usr/local/lib/python3.11/dist-packages (from mistral-common>=1.3.1->-r /content/mistral-finetune/requirements.txt (line 4)) (2.11.2)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from mistral-common>=1.3.1->-r /content/mistral-finetune/requirements.txt (line 4)) (2.32.3)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from mistral-common>=1.3.1->-r /content/mistral-finetune/requirements.txt (line 4)) (0.2.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from mistral-common>=1.3.1->-r /content/mistral-finetune/requirements.txt (line 4)) (0.9.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (1.71.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (3.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (5.29.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (3.1.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral-common>=1.3.1->-r /content/mistral-finetune/requirements.txt (line 4)) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral-common>=1.3.1->-r /content/mistral-finetune/requirements.txt (line 4)) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral-common>=1.3.1->-r /content/mistral-finetune/requirements.txt (line 4)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral-common>=1.3.1->-r /content/mistral-finetune/requirements.txt (line 4)) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=2.7->mistral-common>=1.3.1->-r /content/mistral-finetune/requirements.txt (line 4)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=2.7->mistral-common>=1.3.1->-r /content/mistral-finetune/requirements.txt (line 4)) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=2.7->mistral-common>=1.3.1->-r /content/mistral-finetune/requirements.txt (line 4)) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->mistral-common>=1.3.1->-r /content/mistral-finetune/requirements.txt (line 4)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->mistral-common>=1.3.1->-r /content/mistral-finetune/requirements.txt (line 4)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->mistral-common>=1.3.1->-r /content/mistral-finetune/requirements.txt (line 4)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->mistral-common>=1.3.1->-r /content/mistral-finetune/requirements.txt (line 4)) (2025.1.31)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.7.0->mistral-common>=1.3.1->-r /content/mistral-finetune/requirements.txt (line 4)) (2024.11.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9)) (1.3.0)\n",
            "Using cached torch-2.2.0-cp311-cp311-manylinux1_x86_64.whl (755.5 MB)\n",
            "Using cached triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.3.1.170\n",
            "    Uninstalling nvidia-cusparse-cu12-12.3.1.170:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.5.147\n",
            "    Uninstalling nvidia-curand-cu12-10.3.5.147:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.5.147\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.1.3\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.1.3:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.1.3\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.4.5.8\n",
            "    Uninstalling nvidia-cublas-cu12-12.4.5.8:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.4.5.8\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.1.9\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.1.9:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
            "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0\n",
            "    Uninstalling torch-2.6.0:\n",
            "      Successfully uninstalled torch-2.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.2.0 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvtx-cu12-12.1.105 torch-2.2.0 triton-2.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -r /content/mistral-finetune/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgdIAi257jLo"
      },
      "source": [
        "## Model download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkRSd5UZ7R4q",
        "outputId": "8c449485-32fa-4980-89ca-87b0ebe401c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.30.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "4f3682060fdf4a049ac322b66feab150",
            "b61bc4cd66144d6e909f0a167f590940",
            "b4f8f0547d4746a482a2f747ea9a7402",
            "ab1620f140414c73842471cb782cc0fd",
            "53cc287bd1024a618d22dbf3ba681a26",
            "771b761bc9534baf95068bcb70b3322d",
            "451d2211a82c48efae1da4ecf5b3fceb",
            "4f3489577d554a059455f4ad4b7e37d9",
            "e98668ed12b74880ae0a9e10ee890776",
            "5f5e36f314604560a0579b8ffe82aac6",
            "46917fde00c2434fbee53512a211b9d0",
            "4d494354113247c0b9e7360b801aafe0",
            "44137a5f09bb48acb8af27774ad007f2",
            "8ff263bc2e154014a800a5cf9489d14c",
            "bec5191b2fb149369a5801bc3f720910",
            "ddf54d61079e43379d73fd364f644a72",
            "e5879853dc564d788985177906d464d3",
            "652e682ec6b34c88a33de00fcf3fc4a6",
            "56d92d0589fc4490a9c7fd32d2541fcb",
            "a8a759511ec04c21a7e97bb7dd88cb68"
          ]
        },
        "id": "BZUM4hXI7R4r",
        "outputId": "139f4c91-52a8-4936-a279-188da079c621"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f3682060fdf4a049ac322b66feab150"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# huggingface login\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269,
          "referenced_widgets": [
            "1312c0c0e45d48489a429768fcf640e8",
            "2f55ec47a5ad4974a61e1005f5131002",
            "9b02b8b3224c4260ba831121c9ecbb4f",
            "fe01c190021046be819dfd5f1c47804c",
            "285d987545ad4b1bbe7f6ce5d64b1ff1",
            "997432b383ac499bb54de0f132636ca1",
            "48d400da41cd413aa31e7d613da13d38",
            "198a4b423dd34db192863373f08cc7ea",
            "22aabfbfdf09420bbf42a7f4c56f8af5",
            "b95b03f2c4d84f6d9214decc079efe84",
            "e5a59ee8a6e64fc3a133e5a925073a08",
            "a73419dcb04e46dcb39dfa2a86dafd44",
            "644101346cce4924b39e42c7bd85040a",
            "0a431b655d4c43d59842464434d60a35",
            "4639f94c4aa8407791b99452e9da21d5",
            "bf0237be9f9e49e4a28d603828ba3a65",
            "bdb936c7af1e44f6b7143509ff560136",
            "d2076b1482d648939b7de87ca60d4e5d",
            "5d7f94a12f404495b639424b6924816c",
            "295d23f05afb4e7ba9f3675f4f717eeb",
            "8382047a0c2a4b2ab7e657f2665380c5",
            "e5e9b4976950424f9765bd3cad6bc35f",
            "226d2b86525945548dd6deebbcbc08ee",
            "9cc4feebd41a429e94a0c486c4b9aa64",
            "505bb26171f345e2aa0d2057813edd52",
            "a4bd417593934a7f950abf7f56b3c710",
            "c1747750c0834bdfa20addc4ee7c26ea",
            "87cf4478879d4693addad51719000417",
            "3a6c6433056646d491dedc4381e1d79f",
            "aad1ceab8e814dc6a95d3bba99942e12",
            "70b53663a89942b38bf3b165d0473dce",
            "de5e686b8f164065a6d6365e2f59498c",
            "6a9825f96c2842e48aa2a06adbdd28a4",
            "99333a3f14324ca0bb68a87cdde4eae9",
            "f928701607494b09a98c3cc21257674d",
            "d4b2fb6088e042b4970be5f3eae9206b",
            "9405a81f4bca4351a735d75db373e034",
            "99466760316643d7a2357fdb2cdd28de",
            "a34e83133f4b41f78736d1158c4a9f2f",
            "073c11995613468bbc2b2c02c1b90822",
            "24bc335a0da94c31995fd1a15c056fcf",
            "3ee56ade9cbe4b6ca25d234bbb2b6651",
            "b27e09fcaf9648908833b7cec447b6bd",
            "b8db052e4316444a9c2c73d0b2190dc0"
          ]
        },
        "id": "qgjAADBFHB0S",
        "outputId": "adc60288-4ad3-4718-a568-b3279b8c4cc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1312c0c0e45d48489a429768fcf640e8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "consolidated.safetensors:   0%|          | 0.00/14.5G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a73419dcb04e46dcb39dfa2a86dafd44"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "params.json:   0%|          | 0.00/202 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "226d2b86525945548dd6deebbcbc08ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model.v3:   0%|          | 0.00/587k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "99333a3f14324ca0bb68a87cdde4eae9"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "from pathlib import Path\n",
        "\n",
        "mistral_models_path = Path.home().joinpath('mistral_models', '7B-v0.3')\n",
        "mistral_models_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "snapshot_download(repo_id=\"mistralai/Mistral-7B-v0.3\", allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tokenizer.model.v3\"], local_dir=mistral_models_path)\n",
        "\n",
        "! cp -r /root/mistral_models/7B-v0.3 /content/mistral_models\n",
        "! rm -r /root/mistral_models/7B-v0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cdl_R5baUyha"
      },
      "outputs": [],
      "source": [
        "# Alternatively, you can download the model from mistral\n",
        "\n",
        "# !wget https://models.mistralcdn.com/mistral-7b-v0-3/mistral-7B-v0.3.tar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IgJWR-fReilz"
      },
      "outputs": [],
      "source": [
        "# !DIR=/content/mistral_models && mkdir -p $DIR && tar -xf mistral-7B-v0.3.tar -C $DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PxYGmcy4gu0",
        "outputId": "d9a47d17-4c5f-46f9-b50f-7abdceba2efc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7B-v0.3  consolidated.safetensors  params.json\ttokenizer.model.v3\n"
          ]
        }
      ],
      "source": [
        "!ls /content/mistral_models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ams-19wF8zgY"
      },
      "source": [
        "## Prepare dataset\n",
        "\n",
        "To ensure effective training, mistral-finetune has strict requirements for how the training data has to be formatted. Check out the required data formatting [here](https://github.com/mistralai/mistral-finetune/tree/main?tab=readme-ov-file#prepare-dataset).\n",
        "\n",
        "In this example, let’s use the ultrachat_200k dataset. We load a chunk of the data into Pandas Dataframes, split the data into training and validation, and save the data into the required `jsonl` format for fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T33N2SwCIhEl",
        "outputId": "346a6ffa-9d54-4621-e570-77331c6b6ec2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "i7bmgXvG1vUq"
      },
      "outputs": [],
      "source": [
        "# make a new directory called data\n",
        "!mkdir -p data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Br2czKwwFLE8",
        "outputId": "5db9bb0d-b9dd-46a9-c318-f7a0ae075bee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/data\n"
          ]
        }
      ],
      "source": [
        "# navigate to this data directory\n",
        "%cd /content/data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install numpy==2\n"
      ],
      "metadata": {
        "id": "LdCVIWvIGLJC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch --force-reinstall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vmqe_2DDLGbk",
        "outputId": "f1e9fcdc-1ec6-4925-a2da-a7aeaf3d5ec1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch\n",
            "  Using cached torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting filelock (from torch)\n",
            "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting typing-extensions>=4.10.0 (from torch)\n",
            "  Using cached typing_extensions-4.13.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting networkx (from torch)\n",
            "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2 (from torch)\n",
            "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Using cached fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch)\n",
            "  Using cached nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.2.0 (from torch)\n",
            "  Using cached triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting sympy==1.13.1 (from torch)\n",
            "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
            "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
            "  Using cached MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Using cached torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n",
            "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "Using cached nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
            "Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "Using cached triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
            "Using cached typing_extensions-4.13.1-py3-none-any.whl (45 kB)\n",
            "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
            "Using cached fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
            "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "Using cached MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
            "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "Installing collected packages: triton, nvidia-cusparselt-cu12, mpmath, typing-extensions, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, MarkupSafe, fsspec, filelock, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.2.0\n",
            "    Uninstalling triton-2.2.0:\n",
            "      Successfully uninstalled triton-2.2.0\n",
            "  Attempting uninstall: nvidia-cusparselt-cu12\n",
            "    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n",
            "    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n",
            "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n",
            "  Attempting uninstall: mpmath\n",
            "    Found existing installation: mpmath 1.3.0\n",
            "    Uninstalling mpmath-1.3.0:\n",
            "      Successfully uninstalled mpmath-1.3.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.13.1\n",
            "    Uninstalling typing_extensions-4.13.1:\n",
            "      Successfully uninstalled typing_extensions-4.13.1\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.1.105\n",
            "    Uninstalling nvidia-nvtx-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.19.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.19.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.19.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.2.106\n",
            "    Uninstalling nvidia-curand-cu12-10.3.2.106:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.2.106\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.0.2.54\n",
            "    Uninstalling nvidia-cufft-cu12-11.0.2.54:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.0.2.54\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.1.3.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.1.3.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.1.3.1\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.4.2\n",
            "    Uninstalling networkx-3.4.2:\n",
            "      Successfully uninstalled networkx-3.4.2\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.18.0\n",
            "    Uninstalling filelock-3.18.0:\n",
            "      Successfully uninstalled filelock-3.18.0\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.1.0.106\n",
            "    Uninstalling nvidia-cusparse-cu12-12.1.0.106:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.1.0.106\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 8.9.2.26\n",
            "    Uninstalling nvidia-cudnn-cu12-8.9.2.26:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-8.9.2.26\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.6\n",
            "    Uninstalling Jinja2-3.1.6:\n",
            "      Successfully uninstalled Jinja2-3.1.6\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.4.5.107\n",
            "    Uninstalling nvidia-cusolver-cu12-11.4.5.107:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.4.5.107\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.2.0\n",
            "    Uninstalling torch-2.2.0:\n",
            "      Successfully uninstalled torch-2.2.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xformers 0.0.24 requires torch==2.2.0, but you have torch 2.6.0 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-3.0.2 filelock-3.18.0 fsspec-2025.3.2 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.6.0 triton-3.2.0 typing-extensions-4.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "0rsD7oy8LIj9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install a compatible numpy version (change if a different version is required)\n",
        "import pandas as pd\n",
        "import json\n",
        "# Load your custom JSONL dataset\n",
        "df = pd.read_json('/content/bargaining_scenarios_enhanced.jsonl', lines=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "nlhlNXhZGU2H",
        "outputId": "edb8dbc8-f65b-4c9c-ee65-1036b99455d2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Cannot convert numpy.ndarray to numpy.ndarray",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-07cd9e951676>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load your custom JSONL dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/bargaining_scenarios_enhanced.jsonl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    813\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 815\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1021\u001b[0m                         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m                         \u001b[0mdata_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1023\u001b[0;31m                         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1024\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m                     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"frame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"series\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1185\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1187\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m             self.obj = DataFrame(\n\u001b[0m\u001b[1;32m   1403\u001b[0m                 \u001b[0mujson_loads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    849\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m                         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m                     arrays, columns, index = nested_data_to_arrays(\n\u001b[0m\u001b[1;32m    852\u001b[0m                         \u001b[0;31m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m                         \u001b[0;31m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mnested_data_to_arrays\u001b[0;34m(data, columns, index, dtype)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m     \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    835\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_list_to_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 837\u001b[0;31m         \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_list_of_dict_to_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    838\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_list_of_series_to_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_list_of_dict_to_arrays\u001b[0;34m(data, columns)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0msort\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m         \u001b[0mpre_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_unique_multiple_list_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m     \u001b[0;31m# assure that they are of the base dict class and not of derived\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mensure_index\u001b[0;34m(index_like, copy)\u001b[0m\n\u001b[1;32m   7645\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mMultiIndex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_like\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7646\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7647\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtupleize_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7648\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7649\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, data, dtype, copy, name, tupleize_cols)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"index must be specified when data is not list-like\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_convert_platform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msubarr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m                 \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mmaybe_convert_platform\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_dtype_obj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_convert_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.maybe_convert_objects\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot convert numpy.ndarray to numpy.ndarray"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "RVF8VqU110sB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51c0ee1f-765d-435d-e7e5-6ce4dca5dbc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'messages': [{'role': 'system', 'content': 'Product: Yoga Mat, Price: ₹2000, Min Discount: 5%, Max Discount: 15%, Stock: 7 | User Orders: 3, Refunded: ₹200, Sentiment: Annoyed'}, {'role': 'assistant', 'content': 'Hi there! You’ve got an eye for our premium Yoga Mat. ₹2000 is already a steal!'}, {'role': 'user', 'content': 'I think ₹2000 is too much. How about ₹1500?'}, {'role': 'assistant', 'content': 'Hmm… I like your spirit! How about ₹1800? But only if you buy now.'}, {'role': 'user', 'content': 'Still high. ₹1700?'}, {'role': 'assistant', 'content': 'Alright! I’ll do ₹1700 with a code valid for 10 mins.'}, {'role': 'assistant', 'content': 'Here’s your discount code: YOGAMAT1700'}]}\n",
            "{'messages': [{'role': 'system', 'content': 'Product: Yoga Mat, Price: ₹2000, Min Discount: 5%, Max Discount: 15%, Stock: 13 | User Orders: 3, Refunded: ₹200, Sentiment: Frustrated'}, {'role': 'assistant', 'content': 'Hi there! You’ve got an eye for our premium Yoga Mat. ₹2000 is already a steal!'}, {'role': 'user', 'content': 'I think ₹2000 is too much. How about ₹1500?'}, {'role': 'assistant', 'content': 'Hmm… I like your spirit! How about ₹1800? But only if you buy now.'}, {'role': 'user', 'content': 'Still high. ₹1700?'}, {'role': 'assistant', 'content': 'Alright! I’ll do ₹1700 with a code valid for 10 mins.'}, {'role': 'assistant', 'content': 'Here’s your discount code: YOGAMAT1700'}]}\n",
            "{'messages': [{'role': 'system', 'content': 'Product: Smartwatch, Price: ₹4500, Min Discount: 5%, Max Discount: 15%, Stock: 1 | User Orders: 3, Refunded: ₹200, Sentiment: Defensive'}, {'role': 'assistant', 'content': 'Hi there! You’ve got an eye for our premium Smartwatch. ₹4500 is already a steal!'}, {'role': 'user', 'content': 'I think ₹4500 is too much. How about ₹3375?'}, {'role': 'assistant', 'content': 'Hmm… I like your spirit! How about ₹4050? But only if you buy now.'}, {'role': 'user', 'content': 'Still high. ₹3825?'}, {'role': 'assistant', 'content': 'Alright! I’ll do ₹3825 with a code valid for 10 mins.'}, {'role': 'assistant', 'content': 'Here’s your discount code: SMARTWATCH3825'}]}\n",
            "{'messages': [{'role': 'system', 'content': 'Product: Bluetooth Speaker, Price: ₹3200, Min Discount: 5%, Max Discount: 15%, Stock: 13 | User Orders: 3, Refunded: ₹200, Sentiment: Frustrated'}, {'role': 'assistant', 'content': 'Hi there! You’ve got an eye for our premium Bluetooth Speaker. ₹3200 is already a steal!'}, {'role': 'user', 'content': 'I think ₹3200 is too much. How about ₹2400?'}, {'role': 'assistant', 'content': 'Hmm… I like your spirit! How about ₹2880? But only if you buy now.'}, {'role': 'user', 'content': 'Still high. ₹2720?'}, {'role': 'assistant', 'content': 'Alright! I’ll do ₹2720 with a code valid for 10 mins.'}, {'role': 'assistant', 'content': 'Here’s your discount code: BLUETOOTHSPEAKER2720'}]}\n",
            "{'messages': [{'role': 'system', 'content': 'Product: Designer Sunglasses, Price: ₹5000, Min Discount: 5%, Max Discount: 15%, Stock: 2 | User Orders: 3, Refunded: ₹200, Sentiment: Neutral'}, {'role': 'assistant', 'content': 'Hi there! You’ve got an eye for our premium Designer Sunglasses. ₹5000 is already a steal!'}, {'role': 'user', 'content': 'I think ₹5000 is too much. How about ₹3750?'}, {'role': 'assistant', 'content': 'Hmm… I like your spirit! How about ₹4500? But only if you buy now.'}, {'role': 'user', 'content': 'Still high. ₹4250?'}, {'role': 'assistant', 'content': 'Alright! I’ll do ₹4250 with a code valid for 10 mins.'}, {'role': 'assistant', 'content': 'Here’s your discount code: DESIGNERSUNGLASSES4250'}]}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Read the first few lines of the JSONL file\n",
        "with open('/content/bargaining_scenarios_enhanced.jsonl', 'r') as file:\n",
        "    for _ in range(5):  # Read first 5 lines\n",
        "        print(json.loads(file.readline()))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Read the first few lines to inspect the structure of the JSONL file\n",
        "with open('/content/bargaining_scenarios_enhanced.jsonl', 'r') as file:\n",
        "    for _ in range(5):  # Read and print the first 5 lines\n",
        "        print(json.loads(file.readline()))  # Load each line as JSON and print it\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q27b1VuUHeYf",
        "outputId": "d1c7f25d-97c7-499a-ded7-b1698f48cbc4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'messages': [{'role': 'system', 'content': 'Product: Yoga Mat, Price: ₹2000, Min Discount: 5%, Max Discount: 15%, Stock: 7 | User Orders: 3, Refunded: ₹200, Sentiment: Annoyed'}, {'role': 'assistant', 'content': 'Hi there! You’ve got an eye for our premium Yoga Mat. ₹2000 is already a steal!'}, {'role': 'user', 'content': 'I think ₹2000 is too much. How about ₹1500?'}, {'role': 'assistant', 'content': 'Hmm… I like your spirit! How about ₹1800? But only if you buy now.'}, {'role': 'user', 'content': 'Still high. ₹1700?'}, {'role': 'assistant', 'content': 'Alright! I’ll do ₹1700 with a code valid for 10 mins.'}, {'role': 'assistant', 'content': 'Here’s your discount code: YOGAMAT1700'}]}\n",
            "{'messages': [{'role': 'system', 'content': 'Product: Yoga Mat, Price: ₹2000, Min Discount: 5%, Max Discount: 15%, Stock: 13 | User Orders: 3, Refunded: ₹200, Sentiment: Frustrated'}, {'role': 'assistant', 'content': 'Hi there! You’ve got an eye for our premium Yoga Mat. ₹2000 is already a steal!'}, {'role': 'user', 'content': 'I think ₹2000 is too much. How about ₹1500?'}, {'role': 'assistant', 'content': 'Hmm… I like your spirit! How about ₹1800? But only if you buy now.'}, {'role': 'user', 'content': 'Still high. ₹1700?'}, {'role': 'assistant', 'content': 'Alright! I’ll do ₹1700 with a code valid for 10 mins.'}, {'role': 'assistant', 'content': 'Here’s your discount code: YOGAMAT1700'}]}\n",
            "{'messages': [{'role': 'system', 'content': 'Product: Smartwatch, Price: ₹4500, Min Discount: 5%, Max Discount: 15%, Stock: 1 | User Orders: 3, Refunded: ₹200, Sentiment: Defensive'}, {'role': 'assistant', 'content': 'Hi there! You’ve got an eye for our premium Smartwatch. ₹4500 is already a steal!'}, {'role': 'user', 'content': 'I think ₹4500 is too much. How about ₹3375?'}, {'role': 'assistant', 'content': 'Hmm… I like your spirit! How about ₹4050? But only if you buy now.'}, {'role': 'user', 'content': 'Still high. ₹3825?'}, {'role': 'assistant', 'content': 'Alright! I’ll do ₹3825 with a code valid for 10 mins.'}, {'role': 'assistant', 'content': 'Here’s your discount code: SMARTWATCH3825'}]}\n",
            "{'messages': [{'role': 'system', 'content': 'Product: Bluetooth Speaker, Price: ₹3200, Min Discount: 5%, Max Discount: 15%, Stock: 13 | User Orders: 3, Refunded: ₹200, Sentiment: Frustrated'}, {'role': 'assistant', 'content': 'Hi there! You’ve got an eye for our premium Bluetooth Speaker. ₹3200 is already a steal!'}, {'role': 'user', 'content': 'I think ₹3200 is too much. How about ₹2400?'}, {'role': 'assistant', 'content': 'Hmm… I like your spirit! How about ₹2880? But only if you buy now.'}, {'role': 'user', 'content': 'Still high. ₹2720?'}, {'role': 'assistant', 'content': 'Alright! I’ll do ₹2720 with a code valid for 10 mins.'}, {'role': 'assistant', 'content': 'Here’s your discount code: BLUETOOTHSPEAKER2720'}]}\n",
            "{'messages': [{'role': 'system', 'content': 'Product: Designer Sunglasses, Price: ₹5000, Min Discount: 5%, Max Discount: 15%, Stock: 2 | User Orders: 3, Refunded: ₹200, Sentiment: Neutral'}, {'role': 'assistant', 'content': 'Hi there! You’ve got an eye for our premium Designer Sunglasses. ₹5000 is already a steal!'}, {'role': 'user', 'content': 'I think ₹5000 is too much. How about ₹3750?'}, {'role': 'assistant', 'content': 'Hmm… I like your spirit! How about ₹4500? But only if you buy now.'}, {'role': 'user', 'content': 'Still high. ₹4250?'}, {'role': 'assistant', 'content': 'Alright! I’ll do ₹4250 with a code valid for 10 mins.'}, {'role': 'assistant', 'content': 'Here’s your discount code: DESIGNERSUNGLASSES4250'}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def flatten_json(nested_json):\n",
        "    \"\"\"Flatten a nested JSON object into a flat dictionary.\"\"\"\n",
        "    flat_json = {}\n",
        "\n",
        "    def flatten(d, parent_key=''):\n",
        "        if isinstance(d, dict):\n",
        "            for k, v in d.items():\n",
        "                flatten(v, parent_key + k + '_')  # Add underscores to distinguish nested keys\n",
        "        elif isinstance(d, list):\n",
        "            for i, v in enumerate(d):\n",
        "                flatten(v, parent_key + str(i) + '_')\n",
        "        elif isinstance(d, np.ndarray):\n",
        "            flat_json[parent_key[:-1]] = d.tolist()  # Convert numpy array to list\n",
        "        else:\n",
        "            flat_json[parent_key[:-1]] = d  # Remove trailing underscore\n",
        "    flatten(nested_json)\n",
        "    return flat_json\n",
        "\n",
        "# Now, we will load the JSONL file, flatten it, and convert numpy arrays to lists\n",
        "data = []\n",
        "with open('/content/bargaining_scenarios_enhanced.jsonl', 'r') as file:\n",
        "    for line in file:\n",
        "        json_data = json.loads(line)  # Load each line as JSON\n",
        "        flat_data = flatten_json(json_data)  # Flatten the JSON object and handle numpy arrays\n",
        "        data.append(flat_data)\n",
        "\n",
        "# Convert the flattened data to a pandas DataFrame\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "lgUbHo5DHn_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your custom JSONL dataset\n",
        "df = pd.read_json('/content/bargaining_scenarios_enhanced.jsonl', lines=True)"
      ],
      "metadata": {
        "id": "kwAd2sJaHRQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "YWm53tJTAcDR",
        "outputId": "1e37a3b4-541c-4925-83c2-07514c51e8b9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-00cf07b74dcd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qog1ZEUn12KQ"
      },
      "outputs": [],
      "source": [
        "# split data into training and evaluation\n",
        "df_train=df.sample(frac=0.95,random_state=200)\n",
        "df_eval=df.drop(df_train.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4Yb3NJp13sG"
      },
      "outputs": [],
      "source": [
        "# save data into .jsonl files\n",
        "df_train.to_json(\"ultrachat_chunk_train.jsonl\", orient=\"records\", lines=True)\n",
        "df_eval.to_json(\"ultrachat_chunk_eval.jsonl\", orient=\"records\", lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rc9q_g7EFQLf"
      },
      "outputs": [],
      "source": [
        "!ls /content/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIK0VFXHIn8r",
        "outputId": "bf8c7a46-b920-4863-c7b3-a27d87f7d7c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/mistral-finetune\n"
          ]
        }
      ],
      "source": [
        "# navigate to the mistral-finetune directory\n",
        "%cd /content/mistral-finetune/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "vLHNxpN4GS3i"
      },
      "outputs": [],
      "source": [
        "# some of the training data doesn't have the right format,\n",
        "# so we need to reformat the data into the correct format and skip the cases that don't have the right format:\n",
        "\n",
        "!python -m utils.reformat_data /content/data/ultrachat_chunk_train.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "RscZFo7tGvzS"
      },
      "outputs": [],
      "source": [
        "# eval data looks all good\n",
        "!python -m utils.reformat_data /content/data/ultrachat_chunk_eval.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install weave"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1veFjaggAv1E",
        "outputId": "d4faef1f-63b2-466d-fd7d-26d3393505e7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: weave in /usr/local/lib/python3.11/dist-packages (0.51.41)\n",
            "Requirement already satisfied: diskcache==5.6.3 in /usr/local/lib/python3.11/dist-packages (from weave) (5.6.3)\n",
            "Requirement already satisfied: emoji>=2.12.1 in /usr/local/lib/python3.11/dist-packages (from weave) (2.14.1)\n",
            "Requirement already satisfied: gql[aiohttp,requests] in /usr/local/lib/python3.11/dist-packages (from weave) (3.5.2)\n",
            "Requirement already satisfied: jsonschema>=4.23.0 in /usr/local/lib/python3.11/dist-packages (from weave) (4.23.0)\n",
            "Requirement already satisfied: numpy>1.21.0 in /usr/local/lib/python3.11/dist-packages (from weave) (2.2.4)\n",
            "Requirement already satisfied: packaging>=21.0 in /usr/local/lib/python3.11/dist-packages (from weave) (24.2)\n",
            "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from weave) (2.11.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from weave) (13.9.4)\n",
            "Requirement already satisfied: tenacity!=8.4.0,>=8.3.0 in /usr/local/lib/python3.11/dist-packages (from weave) (9.1.2)\n",
            "Requirement already satisfied: uuid-utils>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from weave) (0.10.0)\n",
            "Requirement already satisfied: wandb>=0.17.1 in /usr/local/lib/python3.11/dist-packages (from weave) (0.19.9)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.23.0->weave) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.23.0->weave) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.23.0->weave) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.23.0->weave) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->weave) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->weave) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->weave) (4.13.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->weave) (0.4.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.1->weave) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.1->weave) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.1->weave) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.1->weave) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.1->weave) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.1->weave) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.1->weave) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.1->weave) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.1->weave) (2.25.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.1->weave) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.1->weave) (75.2.0)\n",
            "Requirement already satisfied: graphql-core<3.2.5,>=3.2 in /usr/local/lib/python3.11/dist-packages (from gql[aiohttp,requests]->weave) (3.2.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.6 in /usr/local/lib/python3.11/dist-packages (from gql[aiohttp,requests]->weave) (1.18.3)\n",
            "Requirement already satisfied: backoff<3.0,>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from gql[aiohttp,requests]->weave) (2.2.1)\n",
            "Requirement already satisfied: anyio<5,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gql[aiohttp,requests]->weave) (4.9.0)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.0 in /usr/local/lib/python3.11/dist-packages (from gql[aiohttp,requests]->weave) (3.11.15)\n",
            "Requirement already satisfied: requests-toolbelt<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from gql[aiohttp,requests]->weave) (1.0.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->weave) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->weave) (2.18.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.0->gql[aiohttp,requests]->weave) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.0->gql[aiohttp,requests]->weave) (1.3.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.0->gql[aiohttp,requests]->weave) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.0->gql[aiohttp,requests]->weave) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.0->gql[aiohttp,requests]->weave) (0.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.0->gql[aiohttp,requests]->weave) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.0->gql[aiohttp,requests]->weave) (1.3.1)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.17.1->weave) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.17.1->weave) (4.0.12)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->weave) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb>=0.17.1->weave) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb>=0.17.1->weave) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb>=0.17.1->weave) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.17.1->weave) (5.0.2)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfV9pE6AA-sQ",
        "outputId": "e57e063d-0c06-497a-e2ae-7828e3565f00"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m211501016\u001b[0m (\u001b[33m211501016-rec\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "# Initialize W&B with a specified project\n",
        "wandb.init(project=\"mistral finetune\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "aEkSunhgBLKO",
        "outputId": "b57ccdda-6923-4959-b3bf-01692e525303"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">legendary-star-2</strong> at: <a href='https://wandb.ai/211501016-rec/my_finetuning_project/runs/6aoen9dq' target=\"_blank\">https://wandb.ai/211501016-rec/my_finetuning_project/runs/6aoen9dq</a><br> View project at: <a href='https://wandb.ai/211501016-rec/my_finetuning_project' target=\"_blank\">https://wandb.ai/211501016-rec/my_finetuning_project</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250408_113511-6aoen9dq/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/mistral-finetune/wandb/run-20250408_113535-f4y613dv</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/211501016-rec/mistral%20finetune/runs/f4y613dv' target=\"_blank\">vague-oath-3</a></strong> to <a href='https://wandb.ai/211501016-rec/mistral%20finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/211501016-rec/mistral%20finetune' target=\"_blank\">https://wandb.ai/211501016-rec/mistral%20finetune</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/211501016-rec/mistral%20finetune/runs/f4y613dv' target=\"_blank\">https://wandb.ai/211501016-rec/mistral%20finetune/runs/f4y613dv</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/211501016-rec/mistral%20finetune/runs/f4y613dv?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x788fcb0ad550>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqhyigF8XVUE",
        "outputId": "d9914e57-af9d-4c6e-c1b3-f695b651f816"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0it [00:00, ?it/s]Validating /content/data/ultrachat_chunk_train.jsonl ...\n",
            "\n",
            "  0% 0/4750 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 240/4750 [00:00<00:01, 2398.60it/s]\u001b[A\n",
            " 10% 480/4750 [00:00<00:01, 2389.04it/s]\u001b[A\n",
            " 15% 719/4750 [00:00<00:01, 2387.63it/s]\u001b[A\n",
            " 20% 959/4750 [00:00<00:01, 2390.68it/s]\u001b[A\n",
            " 25% 1200/4750 [00:00<00:01, 2397.36it/s]\u001b[A\n",
            " 30% 1443/4750 [00:00<00:01, 2408.11it/s]\u001b[A\n",
            " 35% 1684/4750 [00:00<00:01, 2402.00it/s]\u001b[A\n",
            " 41% 1925/4750 [00:00<00:01, 2385.38it/s]\u001b[A\n",
            " 46% 2164/4750 [00:00<00:01, 2380.27it/s]\u001b[A\n",
            " 51% 2406/4750 [00:01<00:00, 2390.46it/s]\u001b[A\n",
            " 56% 2646/4750 [00:01<00:00, 2369.32it/s]\u001b[A\n",
            " 61% 2887/4750 [00:01<00:00, 2379.59it/s]\u001b[A\n",
            " 66% 3130/4750 [00:01<00:00, 2394.29it/s]\u001b[A\n",
            " 71% 3373/4750 [00:01<00:00, 2402.40it/s]\u001b[A\n",
            " 76% 3615/4750 [00:01<00:00, 2404.72it/s]\u001b[A\n",
            " 81% 3858/4750 [00:01<00:00, 2410.84it/s]\u001b[A\n",
            " 86% 4100/4750 [00:01<00:00, 2412.43it/s]\u001b[A\n",
            " 91% 4342/4750 [00:01<00:00, 2401.81it/s]\u001b[A\n",
            "100% 4750/4750 [00:01<00:00, 2397.10it/s]\n",
            "1it [00:01,  1.99s/it]\n",
            "No errors! Data is correctly formatted!\n",
            "Stats for /content/data/ultrachat_chunk_train.jsonl \n",
            " -------------------- \n",
            " {\n",
            "    \"expected\": {\n",
            "        \"eta\": \"00:33:38\",\n",
            "        \"data_tokens\": 932022,\n",
            "        \"train_tokens\": 78643200,\n",
            "        \"epochs\": \"84.38\",\n",
            "        \"max_steps\": 300,\n",
            "        \"data_tokens_per_dataset\": {\n",
            "            \"/content/data/ultrachat_chunk_train.jsonl\": \"932022.0\"\n",
            "        },\n",
            "        \"train_tokens_per_dataset\": {\n",
            "            \"/content/data/ultrachat_chunk_train.jsonl\": \"78643200.0\"\n",
            "        },\n",
            "        \"epochs_per_dataset\": {\n",
            "            \"/content/data/ultrachat_chunk_train.jsonl\": \"84.4\"\n",
            "        }\n",
            "    }\n",
            "}\n",
            "0it [00:00, ?it/s]Validating /content/data/ultrachat_chunk_eval.jsonl ...\n",
            "\n",
            "  0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "100% 250/250 [00:00<00:00, 2395.46it/s]\n",
            "1it [00:00,  9.46it/s]\n",
            "No errors! Data is correctly formatted!\n"
          ]
        }
      ],
      "source": [
        "# Now you can verify your training yaml to make sure the data is correctly formatted and to get an estimate of your training time.\n",
        "\n",
        "!python -m utils.validate_data --train_yaml example/7B.yaml\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hia7n0T1_mHZ"
      },
      "source": [
        "## Start training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ZtcLerooWFeB"
      },
      "outputs": [],
      "source": [
        "# these info is needed for training\n",
        "import os\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "5dxTlIQMaJGv"
      },
      "outputs": [],
      "source": [
        "# define training configuration\n",
        "# for your own use cases, you might want to change the data paths, model path, run_dir, and other hyperparameters\n",
        "\n",
        "config = \"\"\"\n",
        "# data\n",
        "# data\n",
        "data:\n",
        "  instruct_data: \"/content/data/ultrachat_chunk_train.jsonl\"  # Fill\n",
        "  data: \"\"  # Optionally fill with pretraining data\n",
        "  eval_instruct_data: \"/content/data/ultrachat_chunk_eval.jsonl\"  # Optionally fill\n",
        "\n",
        "# model\n",
        "model_id_or_path: \"/content/mistral_models\"  # Change to downloaded path\n",
        "lora:\n",
        "  rank: 64\n",
        "\n",
        "# optim\n",
        "seq_len: 200\n",
        "batch_size: 1\n",
        "max_steps: 300\n",
        "optim:\n",
        "  lr: 6.e-5\n",
        "  weight_decay: 0.1\n",
        "  pct_start: 0.05\n",
        "\n",
        "# other\n",
        "seed: 0\n",
        "log_freq: 1\n",
        "eval_freq: 100\n",
        "no_eval: False\n",
        "ckpt_freq: 100\n",
        "\n",
        "save_adapters: True  # save only trained LoRA adapters. Set to `False` to merge LoRA adapter into the base model and save full fine-tuned model\n",
        "\n",
        "run_dir: \"tune_model\"  # Fill\n",
        "\n",
        "wandb:\n",
        "  project: \"mistral finetune\" # your wandb project name\n",
        "  run_name: \"\" # your wandb run name\n",
        "  key: \"ea6f25bb96b12a5ef05a9a5141138a5244f7438d\" # your wandb api key\n",
        "  offline: False\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# save the same file locally into the example.yaml file\n",
        "import yaml\n",
        "with open('example.yaml', 'w') as file:\n",
        "    yaml.dump(yaml.safe_load(config), file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErD1ktQUMyPZ"
      },
      "outputs": [],
      "source": [
        "# make sure the run_dir has not been created before\n",
        "# only run this when you ran torchrun previously and created the /content/test_ultra file\n",
        "# ! rm -r /content/test_ultra"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip uninstall -y numpy\n"
      ],
      "metadata": {
        "id": "kT8fO5MIEy0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install numpy==1.24.4\n"
      ],
      "metadata": {
        "id": "hCDwWaPXE7Bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print(np.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0w8KIubyE_yO",
        "outputId": "425f4f5b-5bb1-4165-f811-66a30180c6ad"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.23.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y xformers\n",
        "!pip install xformers\n"
      ],
      "metadata": {
        "id": "nF0_zesOMGL2",
        "outputId": "c91923dd-6709-47a7-af18-e79d94c0a59b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: xformers 0.0.24\n",
            "Uninstalling xformers-0.0.24:\n",
            "  Successfully uninstalled xformers-0.0.24\n",
            "Collecting xformers\n",
            "  Downloading xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xformers) (2.2.4)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from xformers) (2.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->xformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->xformers) (3.0.2)\n",
            "Downloading xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl (43.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.4/43.4 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xformers\n",
            "Successfully installed xformers-0.0.29.post3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.2.0+cu121\n",
        "!pip install triton\n"
      ],
      "metadata": {
        "id": "IDGj9QZfMJcT",
        "outputId": "46a47b42-8686-4bdd-98cd-85341b071aa7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==2.2.0+cu121 (from versions: 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==2.2.0+cu121\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version\n"
      ],
      "metadata": {
        "id": "juJFmIXyMOTR",
        "outputId": "87d1bea3-46ec-4a96-ddc6-aa1780b7c47d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.version.cuda)\n"
      ],
      "metadata": {
        "id": "Rb-r1xnxMQ66",
        "outputId": "cdb44555-8a1b-4592-99cc-5665eec61db4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/new_fine_tuned_model\n"
      ],
      "metadata": {
        "id": "_E4REzeRMdFL"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/new_fine_tuned_model\n"
      ],
      "metadata": {
        "id": "z1JUfvr-MooM"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4wFgmwIUTtg",
        "outputId": "949a41b5-dca5-4454-8aa2-f2edb419d1d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-08 11:43:30.493393: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-08 11:43:30.513208: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744112610.535871   24182 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744112610.542823   24182 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-08 11:43:30.566184: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "args: TrainArgs(data=DataArgs(data='', shuffle=False, instruct_data='/content/data/ultrachat_chunk_train.jsonl', eval_instruct_data='/content/data/ultrachat_chunk_eval.jsonl', instruct=InstructArgs(shuffle=True, dynamic_chunk_fn_call=True)), model_id_or_path='/content/mistral_models', run_dir='tune_model', optim=OptimArgs(lr=6e-05, weight_decay=0.1, pct_start=0.05), seed=0, num_microbatches=1, seq_len=250, batch_size=1, max_norm=1.0, max_steps=300, log_freq=1, ckpt_freq=100, save_adapters=True, no_ckpt=False, num_ckpt_keep=3, eval_freq=100, no_eval=False, checkpoint=True, world_size=1, wandb=WandbArgs(project='mistral finetune', offline=False, key='ea6f25bb96b12a5ef05a9a5141138a5244f7438d', run_name=''), mlflow=MLFlowArgs(tracking_uri=None, experiment_name=None), lora=LoraArgs(enable=True, rank=64, dropout=0.0, scaling=2.0))\n",
            "2025-04-08 11:43:34 (UTC) - 0:00:08 - distributed - INFO - torch.cuda.device_count: 1\n",
            "2025-04-08 11:43:34 (UTC) - 0:00:08 - distributed - INFO - CUDA_VISIBLE_DEVICES: 0\n",
            "2025-04-08 11:43:34 (UTC) - 0:00:08 - distributed - INFO - local rank: 0\n",
            "2025-04-08 11:43:34 (UTC) - 0:00:08 - train - INFO - Going to init comms...\n",
            "2025-04-08 11:43:34 (UTC) - 0:00:08 - train - INFO - Run dir: tune_model\n",
            "[rank0]:[W408 11:43:34.706325206 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
            "2025-04-08 11:43:35 (UTC) - 0:00:08 - train - INFO - TrainArgs: {'batch_size': 1,\n",
            " 'checkpoint': True,\n",
            " 'ckpt_freq': 100,\n",
            " 'data': {'data': '',\n",
            "          'eval_instruct_data': '/content/data/ultrachat_chunk_eval.jsonl',\n",
            "          'instruct': {'dynamic_chunk_fn_call': True, 'shuffle': True},\n",
            "          'instruct_data': '/content/data/ultrachat_chunk_train.jsonl',\n",
            "          'shuffle': False},\n",
            " 'eval_freq': 100,\n",
            " 'log_freq': 1,\n",
            " 'lora': {'dropout': 0.0, 'enable': True, 'rank': 64, 'scaling': 2.0},\n",
            " 'max_norm': 1.0,\n",
            " 'max_steps': 300,\n",
            " 'mlflow': {'experiment_name': None, 'tracking_uri': None},\n",
            " 'model_id_or_path': '/content/mistral_models',\n",
            " 'no_ckpt': False,\n",
            " 'no_eval': False,\n",
            " 'num_ckpt_keep': 3,\n",
            " 'num_microbatches': 1,\n",
            " 'optim': {'lr': 6e-05, 'pct_start': 0.05, 'weight_decay': 0.1},\n",
            " 'run_dir': 'tune_model',\n",
            " 'save_adapters': True,\n",
            " 'seed': 0,\n",
            " 'seq_len': 250,\n",
            " 'wandb': {'key': 'ea6f25bb96b12a5ef05a9a5141138a5244f7438d',\n",
            "           'offline': False,\n",
            "           'project': 'mistral finetune',\n",
            "           'run_name': ''},\n",
            " 'world_size': 1}\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m211501016\u001b[0m (\u001b[33m211501016-rec\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "2025-04-08 11:43:35 (UTC) - 0:00:08 - metrics_logger - INFO - initializing wandb\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1mtune_model/wandb/run-20250408_114335-0o4gzp8v\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtune_model\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/211501016-rec/mistral%20finetune\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/211501016-rec/mistral%20finetune/runs/0o4gzp8v\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
            "2025-04-08 11:43:36 (UTC) - 0:00:10 - finetune.wrapped_model - INFO - Reloading model from /content/mistral_models/consolidated.safetensors ...\n",
            "2025-04-08 11:43:36 (UTC) - 0:00:10 - finetune.wrapped_model - INFO - Converting model to dtype torch.bfloat16 ...\n",
            "2025-04-08 11:43:36 (UTC) - 0:00:10 - finetune.wrapped_model - INFO - Loaded model on cpu!\n",
            "2025-04-08 11:43:36 (UTC) - 0:00:10 - finetune.wrapped_model - INFO - Initializing lora layers ...\n",
            "2025-04-08 11:43:37 (UTC) - 0:00:11 - finetune.wrapped_model - INFO - Finished initialization!\n",
            "2025-04-08 11:43:37 (UTC) - 0:00:11 - finetune.wrapped_model - INFO - Sharding model over 1 GPUs ...\n",
            "/usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/_init_utils.py:444: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.FULL_SHARD since the world size is 1.\n",
            "  warnings.warn(\n",
            "2025-04-08 11:43:42 (UTC) - 0:00:15 - finetune.wrapped_model - INFO - Model sharded!\n",
            "2025-04-08 11:43:42 (UTC) - 0:00:15 - finetune.wrapped_model - INFO - 167,772,160 out of 7,415,795,712 parameters are finetuned (2.26%).\n",
            "2025-04-08 11:43:42 (UTC) - 0:00:16 - dataset - INFO - Loading /content/data/ultrachat_chunk_train.jsonl ...\n",
            "2025-04-08 11:43:45 (UTC) - 0:00:18 - dataset - INFO - /content/data/ultrachat_chunk_train.jsonl loaded and tokenized.\n",
            "2025-04-08 11:43:45 (UTC) - 0:00:18 - dataset - INFO - Shuffling /content/data/ultrachat_chunk_train.jsonl ...\n",
            "2025-04-08 11:43:47 (UTC) - 0:00:20 - train - INFO - step: 000001 - done (%): 0.3 - loss: 2.938 - lr: 2.4e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 57.8 - avg_words_per_second: 57.8 - ETA: >2025-04-08 12:05:19\n",
            "2025-04-08 11:43:48 (UTC) - 0:00:21 - train - INFO - step: 000002 - done (%): 0.7 - loss: 2.749 - lr: 3.1e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.4 - avg_words_per_second: 89.1 - ETA: >2025-04-08 11:57:44\n",
            "2025-04-08 11:43:49 (UTC) - 0:00:23 - train - INFO - step: 000003 - done (%): 1.0 - loss: 2.960 - lr: 5.3e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.2 - avg_words_per_second: 108.9 - ETA: >2025-04-08 11:55:11\n",
            "2025-04-08 11:43:51 (UTC) - 0:00:24 - train - INFO - step: 000004 - done (%): 1.3 - loss: 2.648 - lr: 8.7e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 198.0 - avg_words_per_second: 122.7 - ETA: >2025-04-08 11:53:54\n",
            "2025-04-08 11:43:52 (UTC) - 0:00:25 - train - INFO - step: 000005 - done (%): 1.7 - loss: 2.379 - lr: 1.3e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.2 - avg_words_per_second: 132.5 - ETA: >2025-04-08 11:53:08\n",
            "2025-04-08 11:43:53 (UTC) - 0:00:27 - train - INFO - step: 000006 - done (%): 2.0 - loss: 2.147 - lr: 1.9e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.5 - avg_words_per_second: 139.9 - ETA: >2025-04-08 11:52:38\n",
            "2025-04-08 11:43:54 (UTC) - 0:00:28 - train - INFO - step: 000007 - done (%): 2.3 - loss: 1.703 - lr: 2.5e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 190.4 - avg_words_per_second: 145.5 - ETA: >2025-04-08 11:52:18\n",
            "2025-04-08 11:43:56 (UTC) - 0:00:29 - train - INFO - step: 000008 - done (%): 2.7 - loss: 1.565 - lr: 3.1e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.6 - avg_words_per_second: 150.3 - ETA: >2025-04-08 11:52:01\n",
            "2025-04-08 11:43:57 (UTC) - 0:00:30 - train - INFO - step: 000009 - done (%): 3.0 - loss: 1.005 - lr: 3.8e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.4 - avg_words_per_second: 154.2 - ETA: >2025-04-08 11:51:49\n",
            "2025-04-08 11:43:58 (UTC) - 0:00:32 - train - INFO - step: 000010 - done (%): 3.3 - loss: 0.978 - lr: 4.4e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.0 - avg_words_per_second: 157.5 - ETA: >2025-04-08 11:51:39\n",
            "2025-04-08 11:44:00 (UTC) - 0:00:33 - train - INFO - step: 000011 - done (%): 3.7 - loss: 0.654 - lr: 4.9e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.4 - avg_words_per_second: 160.2 - ETA: >2025-04-08 11:51:31\n",
            "2025-04-08 11:44:01 (UTC) - 0:00:34 - train - INFO - step: 000012 - done (%): 4.0 - loss: 0.597 - lr: 5.4e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.3 - avg_words_per_second: 162.6 - ETA: >2025-04-08 11:51:24\n",
            "2025-04-08 11:44:02 (UTC) - 0:00:36 - train - INFO - step: 000013 - done (%): 4.3 - loss: 0.385 - lr: 5.7e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 154.8 - avg_words_per_second: 162.0 - ETA: >2025-04-08 11:51:25\n",
            "2025-04-08 11:44:04 (UTC) - 0:00:37 - train - INFO - step: 000014 - done (%): 4.7 - loss: 0.366 - lr: 5.9e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.8 - avg_words_per_second: 163.9 - ETA: >2025-04-08 11:51:20\n",
            "2025-04-08 11:44:05 (UTC) - 0:00:39 - train - INFO - step: 000015 - done (%): 5.0 - loss: 0.562 - lr: 6.0e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.1 - avg_words_per_second: 165.6 - ETA: >2025-04-08 11:51:15\n",
            "2025-04-08 11:44:06 (UTC) - 0:00:40 - train - INFO - step: 000016 - done (%): 5.3 - loss: 0.259 - lr: 6.0e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 189.2 - avg_words_per_second: 166.9 - ETA: >2025-04-08 11:51:12\n",
            "2025-04-08 11:44:08 (UTC) - 0:00:41 - train - INFO - step: 000017 - done (%): 5.7 - loss: 0.261 - lr: 6.0e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.4 - avg_words_per_second: 168.3 - ETA: >2025-04-08 11:51:08\n",
            "2025-04-08 11:44:09 (UTC) - 0:00:42 - train - INFO - step: 000018 - done (%): 6.0 - loss: 0.393 - lr: 6.0e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 192.5 - avg_words_per_second: 169.5 - ETA: >2025-04-08 11:51:05\n",
            "2025-04-08 11:44:10 (UTC) - 0:00:44 - train - INFO - step: 000019 - done (%): 6.3 - loss: 0.214 - lr: 6.0e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.1 - avg_words_per_second: 170.6 - ETA: >2025-04-08 11:51:02\n",
            "2025-04-08 11:44:12 (UTC) - 0:00:45 - train - INFO - step: 000020 - done (%): 6.7 - loss: 0.114 - lr: 6.0e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.1 - avg_words_per_second: 171.6 - ETA: >2025-04-08 11:50:59\n",
            "2025-04-08 11:44:13 (UTC) - 0:00:46 - train - INFO - step: 000021 - done (%): 7.0 - loss: 0.124 - lr: 6.0e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.7 - avg_words_per_second: 172.6 - ETA: >2025-04-08 11:50:57\n",
            "2025-04-08 11:44:14 (UTC) - 0:00:48 - train - INFO - step: 000022 - done (%): 7.3 - loss: 0.482 - lr: 6.0e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 192.0 - avg_words_per_second: 173.4 - ETA: >2025-04-08 11:50:55\n",
            "2025-04-08 11:44:15 (UTC) - 0:00:49 - train - INFO - step: 000023 - done (%): 7.7 - loss: 0.281 - lr: 6.0e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.3 - avg_words_per_second: 174.2 - ETA: >2025-04-08 11:50:53\n",
            "2025-04-08 11:44:17 (UTC) - 0:00:50 - train - INFO - step: 000024 - done (%): 8.0 - loss: 0.226 - lr: 6.0e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 192.8 - avg_words_per_second: 174.9 - ETA: >2025-04-08 11:50:51\n",
            "2025-04-08 11:44:18 (UTC) - 0:00:52 - train - INFO - step: 000025 - done (%): 8.3 - loss: 0.328 - lr: 6.0e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 188.5 - avg_words_per_second: 175.4 - ETA: >2025-04-08 11:50:50\n",
            "2025-04-08 11:44:19 (UTC) - 0:00:53 - train - INFO - step: 000026 - done (%): 8.7 - loss: 0.130 - lr: 6.0e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 191.2 - avg_words_per_second: 176.0 - ETA: >2025-04-08 11:50:49\n",
            "2025-04-08 11:44:21 (UTC) - 0:00:54 - train - INFO - step: 000027 - done (%): 9.0 - loss: 0.205 - lr: 6.0e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.8 - avg_words_per_second: 176.6 - ETA: >2025-04-08 11:50:47\n",
            "2025-04-08 11:44:22 (UTC) - 0:00:56 - train - INFO - step: 000028 - done (%): 9.3 - loss: 0.092 - lr: 6.0e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 154.3 - avg_words_per_second: 175.7 - ETA: >2025-04-08 11:50:49\n",
            "2025-04-08 11:44:24 (UTC) - 0:00:57 - train - INFO - step: 000029 - done (%): 9.7 - loss: 0.277 - lr: 6.0e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.0 - avg_words_per_second: 176.3 - ETA: >2025-04-08 11:50:48\n",
            "2025-04-08 11:44:25 (UTC) - 0:00:58 - train - INFO - step: 000030 - done (%): 10.0 - loss: 0.034 - lr: 6.0e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.1 - avg_words_per_second: 176.8 - ETA: >2025-04-08 11:50:47\n",
            "2025-04-08 11:44:26 (UTC) - 0:01:00 - train - INFO - step: 000031 - done (%): 10.3 - loss: 0.119 - lr: 6.0e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 191.9 - avg_words_per_second: 177.3 - ETA: >2025-04-08 11:50:45\n",
            "2025-04-08 11:44:27 (UTC) - 0:01:01 - train - INFO - step: 000032 - done (%): 10.7 - loss: 0.048 - lr: 5.9e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.5 - avg_words_per_second: 177.8 - ETA: >2025-04-08 11:50:44\n",
            "2025-04-08 11:44:29 (UTC) - 0:01:02 - train - INFO - step: 000033 - done (%): 11.0 - loss: 0.146 - lr: 5.9e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 191.6 - avg_words_per_second: 178.2 - ETA: >2025-04-08 11:50:43\n",
            "2025-04-08 11:44:30 (UTC) - 0:01:04 - train - INFO - step: 000034 - done (%): 11.3 - loss: 0.264 - lr: 5.9e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 190.8 - avg_words_per_second: 178.5 - ETA: >2025-04-08 11:50:43\n",
            "2025-04-08 11:44:31 (UTC) - 0:01:05 - train - INFO - step: 000035 - done (%): 11.7 - loss: 0.169 - lr: 5.9e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 192.0 - avg_words_per_second: 178.9 - ETA: >2025-04-08 11:50:42\n",
            "2025-04-08 11:44:33 (UTC) - 0:01:06 - train - INFO - step: 000036 - done (%): 12.0 - loss: 0.340 - lr: 5.9e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 191.9 - avg_words_per_second: 179.2 - ETA: >2025-04-08 11:50:41\n",
            "2025-04-08 11:44:34 (UTC) - 0:01:07 - train - INFO - step: 000037 - done (%): 12.3 - loss: 0.171 - lr: 5.9e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 196.1 - avg_words_per_second: 179.6 - ETA: >2025-04-08 11:50:40\n",
            "2025-04-08 11:44:35 (UTC) - 0:01:09 - train - INFO - step: 000038 - done (%): 12.7 - loss: 0.039 - lr: 5.9e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.8 - avg_words_per_second: 180.0 - ETA: >2025-04-08 11:50:39\n",
            "2025-04-08 11:44:37 (UTC) - 0:01:10 - train - INFO - step: 000039 - done (%): 13.0 - loss: 0.131 - lr: 5.9e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 191.4 - avg_words_per_second: 180.3 - ETA: >2025-04-08 11:50:39\n",
            "2025-04-08 11:44:38 (UTC) - 0:01:11 - train - INFO - step: 000040 - done (%): 13.3 - loss: 0.216 - lr: 5.9e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 191.5 - avg_words_per_second: 180.5 - ETA: >2025-04-08 11:50:38\n",
            "2025-04-08 11:44:39 (UTC) - 0:01:13 - train - INFO - step: 000041 - done (%): 13.7 - loss: 0.175 - lr: 5.9e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 192.7 - avg_words_per_second: 180.8 - ETA: >2025-04-08 11:50:37\n",
            "2025-04-08 11:44:40 (UTC) - 0:01:14 - train - INFO - step: 000042 - done (%): 14.0 - loss: 0.106 - lr: 5.9e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 191.6 - avg_words_per_second: 181.0 - ETA: >2025-04-08 11:50:37\n",
            "2025-04-08 11:44:42 (UTC) - 0:01:16 - train - INFO - step: 000043 - done (%): 14.3 - loss: 0.226 - lr: 5.9e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 151.0 - avg_words_per_second: 180.2 - ETA: >2025-04-08 11:50:39\n",
            "2025-04-08 11:44:43 (UTC) - 0:01:17 - train - INFO - step: 000044 - done (%): 14.7 - loss: 0.117 - lr: 5.8e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 188.4 - avg_words_per_second: 180.4 - ETA: >2025-04-08 11:50:38\n",
            "2025-04-08 11:44:45 (UTC) - 0:01:18 - train - INFO - step: 000045 - done (%): 15.0 - loss: 0.092 - lr: 5.8e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.4 - avg_words_per_second: 180.7 - ETA: >2025-04-08 11:50:38\n",
            "2025-04-08 11:44:46 (UTC) - 0:01:19 - train - INFO - step: 000046 - done (%): 15.3 - loss: 0.059 - lr: 5.8e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.2 - avg_words_per_second: 180.9 - ETA: >2025-04-08 11:50:37\n",
            "2025-04-08 11:44:47 (UTC) - 0:01:21 - train - INFO - step: 000047 - done (%): 15.7 - loss: 0.280 - lr: 5.8e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.1 - avg_words_per_second: 181.2 - ETA: >2025-04-08 11:50:36\n",
            "2025-04-08 11:44:49 (UTC) - 0:01:22 - train - INFO - step: 000048 - done (%): 16.0 - loss: 0.076 - lr: 5.8e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.7 - avg_words_per_second: 181.5 - ETA: >2025-04-08 11:50:36\n",
            "2025-04-08 11:44:50 (UTC) - 0:01:23 - train - INFO - step: 000049 - done (%): 16.3 - loss: 0.099 - lr: 5.8e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.1 - avg_words_per_second: 181.7 - ETA: >2025-04-08 11:50:35\n",
            "2025-04-08 11:44:51 (UTC) - 0:01:25 - train - INFO - step: 000050 - done (%): 16.7 - loss: 0.087 - lr: 5.8e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.3 - avg_words_per_second: 181.9 - ETA: >2025-04-08 11:50:35\n",
            "2025-04-08 11:44:52 (UTC) - 0:01:26 - train - INFO - step: 000051 - done (%): 17.0 - loss: 0.219 - lr: 5.8e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.7 - avg_words_per_second: 182.2 - ETA: >2025-04-08 11:50:34\n",
            "2025-04-08 11:44:54 (UTC) - 0:01:27 - train - INFO - step: 000052 - done (%): 17.3 - loss: 0.233 - lr: 5.8e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 191.8 - avg_words_per_second: 182.3 - ETA: >2025-04-08 11:50:34\n",
            "2025-04-08 11:44:55 (UTC) - 0:01:29 - train - INFO - step: 000053 - done (%): 17.7 - loss: 0.114 - lr: 5.7e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.9 - avg_words_per_second: 182.5 - ETA: >2025-04-08 11:50:33\n",
            "2025-04-08 11:44:56 (UTC) - 0:01:30 - train - INFO - step: 000054 - done (%): 18.0 - loss: 0.171 - lr: 5.7e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.4 - avg_words_per_second: 182.7 - ETA: >2025-04-08 11:50:33\n",
            "2025-04-08 11:44:58 (UTC) - 0:01:31 - train - INFO - step: 000055 - done (%): 18.3 - loss: 0.029 - lr: 5.7e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 196.1 - avg_words_per_second: 183.0 - ETA: >2025-04-08 11:50:32\n",
            "2025-04-08 11:44:59 (UTC) - 0:01:32 - train - INFO - step: 000056 - done (%): 18.7 - loss: 0.062 - lr: 5.7e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.5 - avg_words_per_second: 183.1 - ETA: >2025-04-08 11:50:32\n",
            "2025-04-08 11:45:00 (UTC) - 0:01:34 - train - INFO - step: 000057 - done (%): 19.0 - loss: 0.140 - lr: 5.7e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 192.3 - avg_words_per_second: 183.3 - ETA: >2025-04-08 11:50:32\n",
            "2025-04-08 11:45:02 (UTC) - 0:01:35 - train - INFO - step: 000058 - done (%): 19.3 - loss: 0.115 - lr: 5.7e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 155.1 - avg_words_per_second: 182.7 - ETA: >2025-04-08 11:50:33\n",
            "2025-04-08 11:45:03 (UTC) - 0:01:37 - train - INFO - step: 000059 - done (%): 19.7 - loss: 0.177 - lr: 5.7e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.0 - avg_words_per_second: 182.9 - ETA: >2025-04-08 11:50:33\n",
            "2025-04-08 11:45:04 (UTC) - 0:01:38 - train - INFO - step: 000060 - done (%): 20.0 - loss: 0.059 - lr: 5.6e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 192.5 - avg_words_per_second: 183.0 - ETA: >2025-04-08 11:50:32\n",
            "2025-04-08 11:45:06 (UTC) - 0:01:39 - train - INFO - step: 000061 - done (%): 20.3 - loss: 0.187 - lr: 5.6e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.6 - avg_words_per_second: 183.2 - ETA: >2025-04-08 11:50:32\n",
            "2025-04-08 11:45:07 (UTC) - 0:01:40 - train - INFO - step: 000062 - done (%): 20.7 - loss: 0.037 - lr: 5.6e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.1 - avg_words_per_second: 183.4 - ETA: >2025-04-08 11:50:31\n",
            "2025-04-08 11:45:08 (UTC) - 0:01:42 - train - INFO - step: 000063 - done (%): 21.0 - loss: 0.065 - lr: 5.6e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 196.6 - avg_words_per_second: 183.6 - ETA: >2025-04-08 11:50:31\n",
            "2025-04-08 11:45:10 (UTC) - 0:01:43 - train - INFO - step: 000064 - done (%): 21.3 - loss: 0.185 - lr: 5.6e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 197.2 - avg_words_per_second: 183.8 - ETA: >2025-04-08 11:50:31\n",
            "2025-04-08 11:45:11 (UTC) - 0:01:44 - train - INFO - step: 000065 - done (%): 21.7 - loss: 0.172 - lr: 5.6e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.4 - avg_words_per_second: 183.9 - ETA: >2025-04-08 11:50:30\n",
            "2025-04-08 11:45:12 (UTC) - 0:01:46 - train - INFO - step: 000066 - done (%): 22.0 - loss: 0.118 - lr: 5.5e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 196.1 - avg_words_per_second: 184.1 - ETA: >2025-04-08 11:50:30\n",
            "2025-04-08 11:45:13 (UTC) - 0:01:47 - train - INFO - step: 000067 - done (%): 22.3 - loss: 0.057 - lr: 5.5e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 196.6 - avg_words_per_second: 184.3 - ETA: >2025-04-08 11:50:29\n",
            "2025-04-08 11:45:15 (UTC) - 0:01:48 - train - INFO - step: 000068 - done (%): 22.7 - loss: 0.225 - lr: 5.5e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 197.3 - avg_words_per_second: 184.5 - ETA: >2025-04-08 11:50:29\n",
            "2025-04-08 11:45:16 (UTC) - 0:01:49 - train - INFO - step: 000069 - done (%): 23.0 - loss: 0.097 - lr: 5.5e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.1 - avg_words_per_second: 184.6 - ETA: >2025-04-08 11:50:29\n",
            "2025-04-08 11:45:17 (UTC) - 0:01:51 - train - INFO - step: 000070 - done (%): 23.3 - loss: 0.035 - lr: 5.5e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 192.9 - avg_words_per_second: 184.7 - ETA: >2025-04-08 11:50:29\n",
            "2025-04-08 11:45:19 (UTC) - 0:01:52 - train - INFO - step: 000071 - done (%): 23.7 - loss: 0.070 - lr: 5.4e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 192.1 - avg_words_per_second: 184.8 - ETA: >2025-04-08 11:50:28\n",
            "2025-04-08 11:45:20 (UTC) - 0:01:53 - train - INFO - step: 000072 - done (%): 24.0 - loss: 0.111 - lr: 5.4e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 196.3 - avg_words_per_second: 185.0 - ETA: >2025-04-08 11:50:28\n",
            "2025-04-08 11:45:21 (UTC) - 0:01:55 - train - INFO - step: 000073 - done (%): 24.3 - loss: 0.151 - lr: 5.4e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 157.4 - avg_words_per_second: 184.5 - ETA: >2025-04-08 11:50:29\n",
            "2025-04-08 11:45:23 (UTC) - 0:01:56 - train - INFO - step: 000074 - done (%): 24.7 - loss: 0.088 - lr: 5.4e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.7 - avg_words_per_second: 184.7 - ETA: >2025-04-08 11:50:29\n",
            "2025-04-08 11:45:24 (UTC) - 0:01:57 - train - INFO - step: 000075 - done (%): 25.0 - loss: 0.186 - lr: 5.4e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.5 - avg_words_per_second: 184.8 - ETA: >2025-04-08 11:50:28\n",
            "2025-04-08 11:45:25 (UTC) - 0:01:59 - train - INFO - step: 000076 - done (%): 25.3 - loss: 0.041 - lr: 5.3e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.5 - avg_words_per_second: 184.9 - ETA: >2025-04-08 11:50:28\n",
            "2025-04-08 11:45:27 (UTC) - 0:02:00 - train - INFO - step: 000077 - done (%): 25.7 - loss: 0.058 - lr: 5.3e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.9 - avg_words_per_second: 185.1 - ETA: >2025-04-08 11:50:28\n",
            "2025-04-08 11:45:28 (UTC) - 0:02:01 - train - INFO - step: 000078 - done (%): 26.0 - loss: 0.035 - lr: 5.3e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 192.6 - avg_words_per_second: 185.1 - ETA: >2025-04-08 11:50:28\n",
            "2025-04-08 11:45:29 (UTC) - 0:02:03 - train - INFO - step: 000079 - done (%): 26.3 - loss: 0.105 - lr: 5.3e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 191.0 - avg_words_per_second: 185.2 - ETA: >2025-04-08 11:50:27\n",
            "2025-04-08 11:45:30 (UTC) - 0:02:04 - train - INFO - step: 000080 - done (%): 26.7 - loss: 0.160 - lr: 5.3e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 190.3 - avg_words_per_second: 185.3 - ETA: >2025-04-08 11:50:27\n",
            "2025-04-08 11:45:32 (UTC) - 0:02:05 - train - INFO - step: 000081 - done (%): 27.0 - loss: 0.052 - lr: 5.2e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.5 - avg_words_per_second: 185.4 - ETA: >2025-04-08 11:50:27\n",
            "2025-04-08 11:45:33 (UTC) - 0:02:06 - train - INFO - step: 000082 - done (%): 27.3 - loss: 0.223 - lr: 5.2e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.5 - avg_words_per_second: 185.5 - ETA: >2025-04-08 11:50:27\n",
            "2025-04-08 11:45:34 (UTC) - 0:02:08 - train - INFO - step: 000083 - done (%): 27.7 - loss: 0.020 - lr: 5.2e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.9 - avg_words_per_second: 185.6 - ETA: >2025-04-08 11:50:27\n",
            "2025-04-08 11:45:36 (UTC) - 0:02:09 - train - INFO - step: 000084 - done (%): 28.0 - loss: 0.041 - lr: 5.2e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.9 - avg_words_per_second: 185.7 - ETA: >2025-04-08 11:50:26\n",
            "2025-04-08 11:45:37 (UTC) - 0:02:10 - train - INFO - step: 000085 - done (%): 28.3 - loss: 0.146 - lr: 5.2e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.1 - avg_words_per_second: 185.8 - ETA: >2025-04-08 11:50:26\n",
            "2025-04-08 11:45:38 (UTC) - 0:02:12 - train - INFO - step: 000086 - done (%): 28.7 - loss: 0.141 - lr: 5.1e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 196.5 - avg_words_per_second: 185.9 - ETA: >2025-04-08 11:50:26\n",
            "2025-04-08 11:45:40 (UTC) - 0:02:13 - train - INFO - step: 000087 - done (%): 29.0 - loss: 0.165 - lr: 5.1e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 155.8 - avg_words_per_second: 185.5 - ETA: >2025-04-08 11:50:27\n",
            "2025-04-08 11:45:41 (UTC) - 0:02:15 - train - INFO - step: 000088 - done (%): 29.3 - loss: 0.255 - lr: 5.1e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 184.1 - avg_words_per_second: 185.5 - ETA: >2025-04-08 11:50:27\n",
            "2025-04-08 11:45:42 (UTC) - 0:02:16 - train - INFO - step: 000089 - done (%): 29.7 - loss: 0.133 - lr: 5.1e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 183.0 - avg_words_per_second: 185.5 - ETA: >2025-04-08 11:50:27\n",
            "2025-04-08 11:45:44 (UTC) - 0:02:17 - train - INFO - step: 000090 - done (%): 30.0 - loss: 0.156 - lr: 5.0e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 196.8 - avg_words_per_second: 185.6 - ETA: >2025-04-08 11:50:27\n",
            "2025-04-08 11:45:45 (UTC) - 0:02:19 - train - INFO - step: 000091 - done (%): 30.3 - loss: 0.122 - lr: 5.0e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.9 - avg_words_per_second: 185.7 - ETA: >2025-04-08 11:50:26\n",
            "2025-04-08 11:45:46 (UTC) - 0:02:20 - train - INFO - step: 000092 - done (%): 30.7 - loss: 0.059 - lr: 5.0e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.2 - avg_words_per_second: 185.8 - ETA: >2025-04-08 11:50:26\n",
            "2025-04-08 11:45:48 (UTC) - 0:02:21 - train - INFO - step: 000093 - done (%): 31.0 - loss: 0.101 - lr: 5.0e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.0 - avg_words_per_second: 185.9 - ETA: >2025-04-08 11:50:26\n",
            "2025-04-08 11:45:49 (UTC) - 0:02:22 - train - INFO - step: 000094 - done (%): 31.3 - loss: 0.067 - lr: 4.9e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.0 - avg_words_per_second: 185.9 - ETA: >2025-04-08 11:50:26\n",
            "2025-04-08 11:45:50 (UTC) - 0:02:24 - train - INFO - step: 000095 - done (%): 31.7 - loss: 0.031 - lr: 4.9e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.5 - avg_words_per_second: 186.0 - ETA: >2025-04-08 11:50:26\n",
            "2025-04-08 11:45:51 (UTC) - 0:02:25 - train - INFO - step: 000096 - done (%): 32.0 - loss: 0.046 - lr: 4.9e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.2 - avg_words_per_second: 186.1 - ETA: >2025-04-08 11:50:26\n",
            "2025-04-08 11:45:53 (UTC) - 0:02:26 - train - INFO - step: 000097 - done (%): 32.3 - loss: 0.112 - lr: 4.9e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 192.6 - avg_words_per_second: 186.2 - ETA: >2025-04-08 11:50:25\n",
            "2025-04-08 11:45:54 (UTC) - 0:02:28 - train - INFO - step: 000098 - done (%): 32.7 - loss: 0.067 - lr: 4.8e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 192.1 - avg_words_per_second: 186.2 - ETA: >2025-04-08 11:50:25\n",
            "2025-04-08 11:45:55 (UTC) - 0:02:29 - train - INFO - step: 000099 - done (%): 33.0 - loss: 0.066 - lr: 4.8e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.1 - avg_words_per_second: 186.3 - ETA: >2025-04-08 11:50:25\n",
            "2025-04-08 11:45:57 (UTC) - 0:02:30 - eval - INFO - Start eval...\n",
            "2025-04-08 11:46:59 (UTC) - 0:03:33 - eval - INFO - Eval finished!\n",
            "2025-04-08 11:46:59 (UTC) - 0:03:33 - train - INFO - step: 000100 - eval_perplexity: 1.066 - eval_loss: 0.093 - train_loss: 0.124\n",
            "2025-04-08 11:46:59 (UTC) - 0:03:33 - train - INFO - step: 000100 - done (%): 33.3 - loss: 0.124 - lr: 4.8e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 3.9 - avg_words_per_second: 127.2 - ETA: >2025-04-08 11:53:32\n",
            "2025-04-08 11:46:59 (UTC) - 0:03:33 - checkpointing - INFO - Dumping checkpoint in tune_model/checkpoints/checkpoint_000100/consolidated using tmp name: tmp.consolidated\n",
            "/usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/_state_dict_utils.py:773: UserWarning: When using ``NO_SHARD`` for ``ShardingStrategy``, full_state_dict willbe returned.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/_state_dict_utils.py:711: UserWarning: When using ``NO_SHARD`` for ``ShardingStrategy``, full_state_dict willbe returned.\n",
            "  warnings.warn(\n",
            "2025-04-08 11:47:00 (UTC) - 0:03:34 - checkpointing - INFO - Done dumping checkpoint in tune_model/checkpoints/checkpoint_000100/consolidated for step: 100\n",
            "2025-04-08 11:47:00 (UTC) - 0:03:34 - checkpointing - INFO - Done deleting checkpoints \n",
            "2025-04-08 11:47:00 (UTC) - 0:03:34 - checkpointing - INFO - Done!\n",
            "2025-04-08 11:47:01 (UTC) - 0:03:35 - train - INFO - step: 000101 - done (%): 33.7 - loss: 0.097 - lr: 4.7e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 191.0 - avg_words_per_second: 127.6 - ETA: >2025-04-08 11:53:31\n",
            "2025-04-08 11:47:03 (UTC) - 0:03:36 - train - INFO - step: 000102 - done (%): 34.0 - loss: 0.074 - lr: 4.7e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 156.0 - avg_words_per_second: 127.8 - ETA: >2025-04-08 11:53:30\n",
            "2025-04-08 11:47:04 (UTC) - 0:03:38 - train - INFO - step: 000103 - done (%): 34.3 - loss: 0.060 - lr: 4.7e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 192.1 - avg_words_per_second: 128.2 - ETA: >2025-04-08 11:53:28\n",
            "2025-04-08 11:47:06 (UTC) - 0:03:39 - train - INFO - step: 000104 - done (%): 34.7 - loss: 0.222 - lr: 4.7e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 190.5 - avg_words_per_second: 128.6 - ETA: >2025-04-08 11:53:27\n",
            "2025-04-08 11:47:07 (UTC) - 0:03:40 - train - INFO - step: 000105 - done (%): 35.0 - loss: 0.029 - lr: 4.6e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.8 - avg_words_per_second: 129.1 - ETA: >2025-04-08 11:53:25\n",
            "2025-04-08 11:47:08 (UTC) - 0:03:42 - train - INFO - step: 000106 - done (%): 35.3 - loss: 0.084 - lr: 4.6e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.4 - avg_words_per_second: 129.5 - ETA: >2025-04-08 11:53:23\n",
            "2025-04-08 11:47:09 (UTC) - 0:03:43 - train - INFO - step: 000107 - done (%): 35.7 - loss: 0.057 - lr: 4.6e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.7 - avg_words_per_second: 129.9 - ETA: >2025-04-08 11:53:21\n",
            "2025-04-08 11:47:11 (UTC) - 0:03:44 - train - INFO - step: 000108 - done (%): 36.0 - loss: 0.107 - lr: 4.6e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 192.2 - avg_words_per_second: 130.3 - ETA: >2025-04-08 11:53:19\n",
            "2025-04-08 11:47:12 (UTC) - 0:03:45 - train - INFO - step: 000109 - done (%): 36.3 - loss: 0.045 - lr: 4.5e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 196.1 - avg_words_per_second: 130.7 - ETA: >2025-04-08 11:53:17\n",
            "2025-04-08 11:47:13 (UTC) - 0:03:47 - train - INFO - step: 000110 - done (%): 36.7 - loss: 0.110 - lr: 4.5e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 192.8 - avg_words_per_second: 131.0 - ETA: >2025-04-08 11:53:16\n",
            "2025-04-08 11:47:15 (UTC) - 0:03:48 - train - INFO - step: 000111 - done (%): 37.0 - loss: 0.073 - lr: 4.5e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.5 - avg_words_per_second: 131.4 - ETA: >2025-04-08 11:53:14\n",
            "2025-04-08 11:47:16 (UTC) - 0:03:49 - train - INFO - step: 000112 - done (%): 37.3 - loss: 0.110 - lr: 4.4e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 191.3 - avg_words_per_second: 131.8 - ETA: >2025-04-08 11:53:13\n",
            "2025-04-08 11:47:17 (UTC) - 0:03:51 - train - INFO - step: 000113 - done (%): 37.7 - loss: 0.223 - lr: 4.4e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 192.8 - avg_words_per_second: 132.2 - ETA: >2025-04-08 11:53:11\n",
            "2025-04-08 11:47:18 (UTC) - 0:03:52 - train - INFO - step: 000114 - done (%): 38.0 - loss: 0.064 - lr: 4.4e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.1 - avg_words_per_second: 132.5 - ETA: >2025-04-08 11:53:09\n",
            "2025-04-08 11:47:20 (UTC) - 0:03:53 - train - INFO - step: 000115 - done (%): 38.3 - loss: 0.232 - lr: 4.4e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.8 - avg_words_per_second: 132.9 - ETA: >2025-04-08 11:53:08\n",
            "2025-04-08 11:47:21 (UTC) - 0:03:55 - train - INFO - step: 000116 - done (%): 38.7 - loss: 0.075 - lr: 4.3e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.5 - avg_words_per_second: 133.3 - ETA: >2025-04-08 11:53:06\n",
            "2025-04-08 11:47:23 (UTC) - 0:03:56 - train - INFO - step: 000117 - done (%): 39.0 - loss: 0.075 - lr: 4.3e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 156.2 - avg_words_per_second: 133.4 - ETA: >2025-04-08 11:53:05\n",
            "2025-04-08 11:47:24 (UTC) - 0:03:57 - train - INFO - step: 000118 - done (%): 39.3 - loss: 0.172 - lr: 4.3e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 196.0 - avg_words_per_second: 133.8 - ETA: >2025-04-08 11:53:04\n",
            "2025-04-08 11:47:25 (UTC) - 0:03:59 - train - INFO - step: 000119 - done (%): 39.7 - loss: 0.084 - lr: 4.2e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.9 - avg_words_per_second: 134.2 - ETA: >2025-04-08 11:53:03\n",
            "2025-04-08 11:47:27 (UTC) - 0:04:00 - train - INFO - step: 000120 - done (%): 40.0 - loss: 0.107 - lr: 4.2e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.6 - avg_words_per_second: 134.5 - ETA: >2025-04-08 11:53:01\n",
            "2025-04-08 11:47:28 (UTC) - 0:04:01 - train - INFO - step: 000121 - done (%): 40.3 - loss: 0.307 - lr: 4.2e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.6 - avg_words_per_second: 134.9 - ETA: >2025-04-08 11:53:00\n",
            "2025-04-08 11:47:29 (UTC) - 0:04:03 - train - INFO - step: 000122 - done (%): 40.7 - loss: 0.132 - lr: 4.1e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 196.4 - avg_words_per_second: 135.2 - ETA: >2025-04-08 11:52:58\n",
            "2025-04-08 11:47:30 (UTC) - 0:04:04 - train - INFO - step: 000123 - done (%): 41.0 - loss: 0.026 - lr: 4.1e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.3 - avg_words_per_second: 135.5 - ETA: >2025-04-08 11:52:57\n",
            "2025-04-08 11:47:32 (UTC) - 0:04:05 - train - INFO - step: 000124 - done (%): 41.3 - loss: 0.086 - lr: 4.1e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 197.0 - avg_words_per_second: 135.9 - ETA: >2025-04-08 11:52:55\n",
            "2025-04-08 11:47:33 (UTC) - 0:04:06 - train - INFO - step: 000125 - done (%): 41.7 - loss: 0.031 - lr: 4.1e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.3 - avg_words_per_second: 136.2 - ETA: >2025-04-08 11:52:54\n",
            "2025-04-08 11:47:34 (UTC) - 0:04:08 - train - INFO - step: 000126 - done (%): 42.0 - loss: 0.031 - lr: 4.0e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 196.2 - avg_words_per_second: 136.5 - ETA: >2025-04-08 11:52:53\n",
            "2025-04-08 11:47:36 (UTC) - 0:04:09 - train - INFO - step: 000127 - done (%): 42.3 - loss: 0.084 - lr: 4.0e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.9 - avg_words_per_second: 136.9 - ETA: >2025-04-08 11:52:52\n",
            "2025-04-08 11:47:37 (UTC) - 0:04:10 - train - INFO - step: 000128 - done (%): 42.7 - loss: 0.054 - lr: 4.0e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.7 - avg_words_per_second: 137.2 - ETA: >2025-04-08 11:52:50\n",
            "2025-04-08 11:47:38 (UTC) - 0:04:12 - train - INFO - step: 000129 - done (%): 43.0 - loss: 0.090 - lr: 3.9e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.6 - avg_words_per_second: 137.5 - ETA: >2025-04-08 11:52:49\n",
            "2025-04-08 11:47:39 (UTC) - 0:04:13 - train - INFO - step: 000130 - done (%): 43.3 - loss: 0.131 - lr: 3.9e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 191.9 - avg_words_per_second: 137.8 - ETA: >2025-04-08 11:52:48\n",
            "2025-04-08 11:47:41 (UTC) - 0:04:14 - train - INFO - step: 000131 - done (%): 43.7 - loss: 0.107 - lr: 3.9e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 189.5 - avg_words_per_second: 138.1 - ETA: >2025-04-08 11:52:47\n",
            "2025-04-08 11:47:42 (UTC) - 0:04:16 - train - INFO - step: 000132 - done (%): 44.0 - loss: 0.054 - lr: 3.8e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 153.6 - avg_words_per_second: 138.2 - ETA: >2025-04-08 11:52:46\n",
            "2025-04-08 11:47:44 (UTC) - 0:04:17 - train - INFO - step: 000133 - done (%): 44.3 - loss: 0.227 - lr: 3.8e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.3 - avg_words_per_second: 138.5 - ETA: >2025-04-08 11:52:45\n",
            "2025-04-08 11:47:45 (UTC) - 0:04:18 - train - INFO - step: 000134 - done (%): 44.7 - loss: 0.028 - lr: 3.8e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.6 - avg_words_per_second: 138.8 - ETA: >2025-04-08 11:52:44\n",
            "2025-04-08 11:47:46 (UTC) - 0:04:20 - train - INFO - step: 000135 - done (%): 45.0 - loss: 0.085 - lr: 3.7e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.2 - avg_words_per_second: 139.1 - ETA: >2025-04-08 11:52:43\n",
            "2025-04-08 11:47:47 (UTC) - 0:04:21 - train - INFO - step: 000136 - done (%): 45.3 - loss: 0.136 - lr: 3.7e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.0 - avg_words_per_second: 139.4 - ETA: >2025-04-08 11:52:42\n",
            "2025-04-08 11:47:49 (UTC) - 0:04:22 - train - INFO - step: 000137 - done (%): 45.7 - loss: 0.059 - lr: 3.7e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.7 - avg_words_per_second: 139.7 - ETA: >2025-04-08 11:52:41\n",
            "2025-04-08 11:47:50 (UTC) - 0:04:24 - train - INFO - step: 000138 - done (%): 46.0 - loss: 0.135 - lr: 3.6e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.3 - avg_words_per_second: 139.9 - ETA: >2025-04-08 11:52:39\n",
            "2025-04-08 11:47:51 (UTC) - 0:04:25 - train - INFO - step: 000139 - done (%): 46.3 - loss: 0.051 - lr: 3.6e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.1 - avg_words_per_second: 140.2 - ETA: >2025-04-08 11:52:38\n",
            "2025-04-08 11:47:53 (UTC) - 0:04:26 - train - INFO - step: 000140 - done (%): 46.7 - loss: 0.205 - lr: 3.6e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 191.0 - avg_words_per_second: 140.5 - ETA: >2025-04-08 11:52:37\n",
            "2025-04-08 11:47:54 (UTC) - 0:04:27 - train - INFO - step: 000141 - done (%): 47.0 - loss: 0.014 - lr: 3.5e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 191.2 - avg_words_per_second: 140.8 - ETA: >2025-04-08 11:52:36\n",
            "2025-04-08 11:47:55 (UTC) - 0:04:29 - train - INFO - step: 000142 - done (%): 47.3 - loss: 0.051 - lr: 3.5e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.4 - avg_words_per_second: 141.0 - ETA: >2025-04-08 11:52:35\n",
            "2025-04-08 11:47:57 (UTC) - 0:04:30 - train - INFO - step: 000143 - done (%): 47.7 - loss: 0.119 - lr: 3.5e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.4 - avg_words_per_second: 141.3 - ETA: >2025-04-08 11:52:34\n",
            "2025-04-08 11:47:58 (UTC) - 0:04:31 - train - INFO - step: 000144 - done (%): 48.0 - loss: 0.092 - lr: 3.4e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 192.9 - avg_words_per_second: 141.6 - ETA: >2025-04-08 11:52:33\n",
            "2025-04-08 11:47:59 (UTC) - 0:04:33 - train - INFO - step: 000145 - done (%): 48.3 - loss: 0.079 - lr: 3.4e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.0 - avg_words_per_second: 141.8 - ETA: >2025-04-08 11:52:32\n",
            "2025-04-08 11:48:01 (UTC) - 0:04:34 - train - INFO - step: 000146 - done (%): 48.7 - loss: 0.040 - lr: 3.4e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 154.9 - avg_words_per_second: 141.9 - ETA: >2025-04-08 11:52:32\n",
            "2025-04-08 11:48:02 (UTC) - 0:04:35 - train - INFO - step: 000147 - done (%): 49.0 - loss: 0.310 - lr: 3.3e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.6 - avg_words_per_second: 142.2 - ETA: >2025-04-08 11:52:31\n",
            "2025-04-08 11:48:03 (UTC) - 0:04:37 - train - INFO - step: 000148 - done (%): 49.3 - loss: 0.042 - lr: 3.3e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 192.9 - avg_words_per_second: 142.4 - ETA: >2025-04-08 11:52:30\n",
            "2025-04-08 11:48:05 (UTC) - 0:04:38 - train - INFO - step: 000149 - done (%): 49.7 - loss: 0.046 - lr: 3.3e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.3 - avg_words_per_second: 142.7 - ETA: >2025-04-08 11:52:29\n",
            "2025-04-08 11:48:06 (UTC) - 0:04:39 - train - INFO - step: 000150 - done (%): 50.0 - loss: 0.059 - lr: 3.2e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.2 - avg_words_per_second: 142.9 - ETA: >2025-04-08 11:52:28\n",
            "2025-04-08 11:48:07 (UTC) - 0:04:41 - train - INFO - step: 000151 - done (%): 50.3 - loss: 0.146 - lr: 3.2e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 196.6 - avg_words_per_second: 143.2 - ETA: >2025-04-08 11:52:27\n",
            "2025-04-08 11:48:08 (UTC) - 0:04:42 - train - INFO - step: 000152 - done (%): 50.7 - loss: 0.088 - lr: 3.2e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 196.3 - avg_words_per_second: 143.5 - ETA: >2025-04-08 11:52:26\n",
            "2025-04-08 11:48:10 (UTC) - 0:04:43 - train - INFO - step: 000153 - done (%): 51.0 - loss: 0.055 - lr: 3.1e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.3 - avg_words_per_second: 143.7 - ETA: >2025-04-08 11:52:25\n",
            "2025-04-08 11:48:11 (UTC) - 0:04:44 - train - INFO - step: 000154 - done (%): 51.3 - loss: 0.146 - lr: 3.1e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.0 - avg_words_per_second: 143.9 - ETA: >2025-04-08 11:52:25\n",
            "2025-04-08 11:48:12 (UTC) - 0:04:46 - train - INFO - step: 000155 - done (%): 51.7 - loss: 0.057 - lr: 3.1e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 198.6 - avg_words_per_second: 144.2 - ETA: >2025-04-08 11:52:24\n",
            "2025-04-08 11:48:14 (UTC) - 0:04:47 - train - INFO - step: 000156 - done (%): 52.0 - loss: 0.047 - lr: 3.0e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 197.2 - avg_words_per_second: 144.5 - ETA: >2025-04-08 11:52:23\n",
            "2025-04-08 11:48:15 (UTC) - 0:04:48 - train - INFO - step: 000157 - done (%): 52.3 - loss: 0.126 - lr: 3.0e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 196.4 - avg_words_per_second: 144.7 - ETA: >2025-04-08 11:52:22\n",
            "2025-04-08 11:48:16 (UTC) - 0:04:50 - train - INFO - step: 000158 - done (%): 52.7 - loss: 0.100 - lr: 3.0e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.6 - avg_words_per_second: 144.9 - ETA: >2025-04-08 11:52:21\n",
            "2025-04-08 11:48:17 (UTC) - 0:04:51 - train - INFO - step: 000159 - done (%): 53.0 - loss: 0.055 - lr: 3.0e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 197.7 - avg_words_per_second: 145.2 - ETA: >2025-04-08 11:52:20\n",
            "2025-04-08 11:48:19 (UTC) - 0:04:52 - train - INFO - step: 000160 - done (%): 53.3 - loss: 0.049 - lr: 2.9e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.3 - avg_words_per_second: 145.4 - ETA: >2025-04-08 11:52:19\n",
            "2025-04-08 11:48:20 (UTC) - 0:04:54 - train - INFO - step: 000161 - done (%): 53.7 - loss: 0.149 - lr: 2.9e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 157.2 - avg_words_per_second: 145.5 - ETA: >2025-04-08 11:52:19\n",
            "2025-04-08 11:48:22 (UTC) - 0:04:55 - train - INFO - step: 000162 - done (%): 54.0 - loss: 0.017 - lr: 2.9e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.9 - avg_words_per_second: 145.7 - ETA: >2025-04-08 11:52:18\n",
            "2025-04-08 11:48:23 (UTC) - 0:04:56 - train - INFO - step: 000163 - done (%): 54.3 - loss: 0.054 - lr: 2.8e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 197.0 - avg_words_per_second: 145.9 - ETA: >2025-04-08 11:52:17\n",
            "2025-04-08 11:48:24 (UTC) - 0:04:58 - train - INFO - step: 000164 - done (%): 54.7 - loss: 0.085 - lr: 2.8e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.4 - avg_words_per_second: 146.2 - ETA: >2025-04-08 11:52:17\n",
            "2025-04-08 11:48:25 (UTC) - 0:04:59 - train - INFO - step: 000165 - done (%): 55.0 - loss: 0.040 - lr: 2.8e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 197.8 - avg_words_per_second: 146.4 - ETA: >2025-04-08 11:52:16\n",
            "2025-04-08 11:48:27 (UTC) - 0:05:00 - train - INFO - step: 000166 - done (%): 55.3 - loss: 0.055 - lr: 2.7e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.4 - avg_words_per_second: 146.6 - ETA: >2025-04-08 11:52:15\n",
            "2025-04-08 11:48:28 (UTC) - 0:05:01 - train - INFO - step: 000167 - done (%): 55.7 - loss: 0.027 - lr: 2.7e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.8 - avg_words_per_second: 146.8 - ETA: >2025-04-08 11:52:14\n",
            "2025-04-08 11:48:29 (UTC) - 0:05:03 - train - INFO - step: 000168 - done (%): 56.0 - loss: 0.118 - lr: 2.7e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 196.0 - avg_words_per_second: 147.0 - ETA: >2025-04-08 11:52:14\n",
            "2025-04-08 11:48:30 (UTC) - 0:05:04 - train - INFO - step: 000169 - done (%): 56.3 - loss: 0.064 - lr: 2.6e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 196.5 - avg_words_per_second: 147.3 - ETA: >2025-04-08 11:52:13\n",
            "2025-04-08 11:48:32 (UTC) - 0:05:05 - train - INFO - step: 000170 - done (%): 56.7 - loss: 0.021 - lr: 2.6e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 192.7 - avg_words_per_second: 147.5 - ETA: >2025-04-08 11:52:12\n",
            "2025-04-08 11:48:33 (UTC) - 0:05:07 - train - INFO - step: 000171 - done (%): 57.0 - loss: 0.025 - lr: 2.6e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.8 - avg_words_per_second: 147.7 - ETA: >2025-04-08 11:52:11\n",
            "2025-04-08 11:48:34 (UTC) - 0:05:08 - train - INFO - step: 000172 - done (%): 57.3 - loss: 0.105 - lr: 2.5e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 196.8 - avg_words_per_second: 147.9 - ETA: >2025-04-08 11:52:11\n",
            "2025-04-08 11:48:36 (UTC) - 0:05:09 - train - INFO - step: 000173 - done (%): 57.7 - loss: 0.061 - lr: 2.5e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.5 - avg_words_per_second: 148.1 - ETA: >2025-04-08 11:52:10\n",
            "2025-04-08 11:48:37 (UTC) - 0:05:10 - train - INFO - step: 000174 - done (%): 58.0 - loss: 0.033 - lr: 2.5e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.7 - avg_words_per_second: 148.3 - ETA: >2025-04-08 11:52:09\n",
            "2025-04-08 11:48:38 (UTC) - 0:05:12 - train - INFO - step: 000175 - done (%): 58.3 - loss: 0.085 - lr: 2.4e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.4 - avg_words_per_second: 148.5 - ETA: >2025-04-08 11:52:09\n",
            "2025-04-08 11:48:40 (UTC) - 0:05:13 - train - INFO - step: 000176 - done (%): 58.7 - loss: 0.066 - lr: 2.4e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 156.9 - avg_words_per_second: 148.6 - ETA: >2025-04-08 11:52:08\n",
            "2025-04-08 11:48:41 (UTC) - 0:05:15 - train - INFO - step: 000177 - done (%): 59.0 - loss: 0.081 - lr: 2.4e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 196.8 - avg_words_per_second: 148.8 - ETA: >2025-04-08 11:52:08\n",
            "2025-04-08 11:48:42 (UTC) - 0:05:16 - train - INFO - step: 000178 - done (%): 59.3 - loss: 0.062 - lr: 2.3e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.4 - avg_words_per_second: 149.0 - ETA: >2025-04-08 11:52:07\n",
            "2025-04-08 11:48:44 (UTC) - 0:05:17 - train - INFO - step: 000179 - done (%): 59.7 - loss: 0.148 - lr: 2.3e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 196.4 - avg_words_per_second: 149.2 - ETA: >2025-04-08 11:52:06\n",
            "2025-04-08 11:48:45 (UTC) - 0:05:18 - train - INFO - step: 000180 - done (%): 60.0 - loss: 0.015 - lr: 2.3e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 197.8 - avg_words_per_second: 149.4 - ETA: >2025-04-08 11:52:06\n",
            "2025-04-08 11:48:46 (UTC) - 0:05:20 - train - INFO - step: 000181 - done (%): 60.3 - loss: 0.031 - lr: 2.2e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 197.1 - avg_words_per_second: 149.6 - ETA: >2025-04-08 11:52:05\n",
            "2025-04-08 11:48:47 (UTC) - 0:05:21 - train - INFO - step: 000182 - done (%): 60.7 - loss: 0.026 - lr: 2.2e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 196.9 - avg_words_per_second: 149.8 - ETA: >2025-04-08 11:52:04\n",
            "2025-04-08 11:48:49 (UTC) - 0:05:22 - train - INFO - step: 000183 - done (%): 61.0 - loss: 0.094 - lr: 2.2e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 197.3 - avg_words_per_second: 150.0 - ETA: >2025-04-08 11:52:04\n",
            "2025-04-08 11:48:50 (UTC) - 0:05:23 - train - INFO - step: 000184 - done (%): 61.3 - loss: 0.021 - lr: 2.1e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 196.6 - avg_words_per_second: 150.2 - ETA: >2025-04-08 11:52:03\n",
            "2025-04-08 11:48:51 (UTC) - 0:05:25 - train - INFO - step: 000185 - done (%): 61.7 - loss: 0.016 - lr: 2.1e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.9 - avg_words_per_second: 150.3 - ETA: >2025-04-08 11:52:02\n",
            "2025-04-08 11:48:53 (UTC) - 0:05:26 - train - INFO - step: 000186 - done (%): 62.0 - loss: 0.258 - lr: 2.1e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.6 - avg_words_per_second: 150.5 - ETA: >2025-04-08 11:52:02\n",
            "2025-04-08 11:48:54 (UTC) - 0:05:27 - train - INFO - step: 000187 - done (%): 62.3 - loss: 0.088 - lr: 2.0e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.6 - avg_words_per_second: 150.7 - ETA: >2025-04-08 11:52:01\n",
            "2025-04-08 11:48:55 (UTC) - 0:05:29 - train - INFO - step: 000188 - done (%): 62.7 - loss: 0.064 - lr: 2.0e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 196.6 - avg_words_per_second: 150.9 - ETA: >2025-04-08 11:52:01\n",
            "2025-04-08 11:48:56 (UTC) - 0:05:30 - train - INFO - step: 000189 - done (%): 63.0 - loss: 0.035 - lr: 2.0e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 196.5 - avg_words_per_second: 151.1 - ETA: >2025-04-08 11:52:00\n",
            "2025-04-08 11:48:58 (UTC) - 0:05:31 - train - INFO - step: 000190 - done (%): 63.3 - loss: 0.118 - lr: 1.9e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 196.8 - avg_words_per_second: 151.3 - ETA: >2025-04-08 11:51:59\n",
            "2025-04-08 11:48:59 (UTC) - 0:05:33 - train - INFO - step: 000191 - done (%): 63.7 - loss: 0.021 - lr: 1.9e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 158.5 - avg_words_per_second: 151.3 - ETA: >2025-04-08 11:51:59\n",
            "2025-04-08 11:49:00 (UTC) - 0:05:34 - train - INFO - step: 000192 - done (%): 64.0 - loss: 0.020 - lr: 1.9e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 197.7 - avg_words_per_second: 151.5 - ETA: >2025-04-08 11:51:59\n",
            "2025-04-08 11:49:02 (UTC) - 0:05:35 - train - INFO - step: 000193 - done (%): 64.3 - loss: 0.175 - lr: 1.9e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.5 - avg_words_per_second: 151.7 - ETA: >2025-04-08 11:51:58\n",
            "2025-04-08 11:49:03 (UTC) - 0:05:36 - train - INFO - step: 000194 - done (%): 64.7 - loss: 0.068 - lr: 1.8e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.9 - avg_words_per_second: 151.8 - ETA: >2025-04-08 11:51:58\n",
            "2025-04-08 11:49:04 (UTC) - 0:05:38 - train - INFO - step: 000195 - done (%): 65.0 - loss: 0.028 - lr: 1.8e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.8 - avg_words_per_second: 152.0 - ETA: >2025-04-08 11:51:57\n",
            "2025-04-08 11:49:06 (UTC) - 0:05:39 - train - INFO - step: 000196 - done (%): 65.3 - loss: 0.092 - lr: 1.8e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.9 - avg_words_per_second: 152.2 - ETA: >2025-04-08 11:51:56\n",
            "2025-04-08 11:49:07 (UTC) - 0:05:40 - train - INFO - step: 000197 - done (%): 65.7 - loss: 0.169 - lr: 1.7e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 198.0 - avg_words_per_second: 152.4 - ETA: >2025-04-08 11:51:56\n",
            "2025-04-08 11:49:08 (UTC) - 0:05:42 - train - INFO - step: 000198 - done (%): 66.0 - loss: 0.071 - lr: 1.7e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.1 - avg_words_per_second: 152.5 - ETA: >2025-04-08 11:51:55\n",
            "2025-04-08 11:49:09 (UTC) - 0:05:43 - train - INFO - step: 000199 - done (%): 66.3 - loss: 0.048 - lr: 1.7e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 196.6 - avg_words_per_second: 152.7 - ETA: >2025-04-08 11:51:55\n",
            "2025-04-08 11:49:11 (UTC) - 0:05:44 - eval - INFO - Start eval...\n",
            "2025-04-08 11:50:13 (UTC) - 0:06:47 - eval - INFO - Eval finished!\n",
            "2025-04-08 11:50:13 (UTC) - 0:06:47 - train - INFO - step: 000200 - eval_perplexity: 1.043 - eval_loss: 0.061 - train_loss: 0.073\n",
            "2025-04-08 11:50:13 (UTC) - 0:06:47 - train - INFO - step: 000200 - done (%): 66.7 - loss: 0.073 - lr: 1.6e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 3.9 - avg_words_per_second: 128.4 - ETA: >2025-04-08 11:53:28\n",
            "2025-04-08 11:50:13 (UTC) - 0:06:47 - checkpointing - INFO - Dumping checkpoint in tune_model/checkpoints/checkpoint_000200/consolidated using tmp name: tmp.consolidated\n",
            "2025-04-08 11:50:14 (UTC) - 0:06:48 - checkpointing - INFO - Done dumping checkpoint in tune_model/checkpoints/checkpoint_000200/consolidated for step: 200\n",
            "2025-04-08 11:50:14 (UTC) - 0:06:48 - checkpointing - INFO - Done deleting checkpoints \n",
            "2025-04-08 11:50:14 (UTC) - 0:06:48 - checkpointing - INFO - Done!\n",
            "2025-04-08 11:50:15 (UTC) - 0:06:49 - train - INFO - step: 000201 - done (%): 67.0 - loss: 0.101 - lr: 1.6e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 189.6 - avg_words_per_second: 128.6 - ETA: >2025-04-08 11:53:28\n",
            "2025-04-08 11:50:17 (UTC) - 0:06:50 - train - INFO - step: 000202 - done (%): 67.3 - loss: 0.155 - lr: 1.6e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.5 - avg_words_per_second: 128.8 - ETA: >2025-04-08 11:53:27\n",
            "2025-04-08 11:50:18 (UTC) - 0:06:51 - train - INFO - step: 000203 - done (%): 67.7 - loss: 0.056 - lr: 1.6e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.0 - avg_words_per_second: 129.0 - ETA: >2025-04-08 11:53:26\n",
            "2025-04-08 11:50:19 (UTC) - 0:06:53 - train - INFO - step: 000204 - done (%): 68.0 - loss: 0.085 - lr: 1.5e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.3 - avg_words_per_second: 129.2 - ETA: >2025-04-08 11:53:25\n",
            "2025-04-08 11:50:21 (UTC) - 0:06:54 - train - INFO - step: 000205 - done (%): 68.3 - loss: 0.014 - lr: 1.5e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 156.2 - avg_words_per_second: 129.3 - ETA: >2025-04-08 11:53:25\n",
            "2025-04-08 11:50:22 (UTC) - 0:06:56 - train - INFO - step: 000206 - done (%): 68.7 - loss: 0.032 - lr: 1.5e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 196.1 - avg_words_per_second: 129.5 - ETA: >2025-04-08 11:53:24\n",
            "2025-04-08 11:50:23 (UTC) - 0:06:57 - train - INFO - step: 000207 - done (%): 69.0 - loss: 0.024 - lr: 1.4e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.5 - avg_words_per_second: 129.7 - ETA: >2025-04-08 11:53:23\n",
            "2025-04-08 11:50:25 (UTC) - 0:06:58 - train - INFO - step: 000208 - done (%): 69.3 - loss: 0.080 - lr: 1.4e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 192.9 - avg_words_per_second: 130.0 - ETA: >2025-04-08 11:53:22\n",
            "2025-04-08 11:50:26 (UTC) - 0:07:00 - train - INFO - step: 000209 - done (%): 69.7 - loss: 0.069 - lr: 1.4e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 190.2 - avg_words_per_second: 130.1 - ETA: >2025-04-08 11:53:21\n",
            "2025-04-08 11:50:27 (UTC) - 0:07:01 - train - INFO - step: 000210 - done (%): 70.0 - loss: 0.030 - lr: 1.4e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.6 - avg_words_per_second: 130.4 - ETA: >2025-04-08 11:53:20\n",
            "2025-04-08 11:50:29 (UTC) - 0:07:02 - train - INFO - step: 000211 - done (%): 70.3 - loss: 0.149 - lr: 1.3e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.9 - avg_words_per_second: 130.6 - ETA: >2025-04-08 11:53:19\n",
            "2025-04-08 11:50:30 (UTC) - 0:07:03 - train - INFO - step: 000212 - done (%): 70.7 - loss: 0.021 - lr: 1.3e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 196.6 - avg_words_per_second: 130.8 - ETA: >2025-04-08 11:53:18\n",
            "2025-04-08 11:50:31 (UTC) - 0:07:05 - train - INFO - step: 000213 - done (%): 71.0 - loss: 0.019 - lr: 1.3e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 196.1 - avg_words_per_second: 131.0 - ETA: >2025-04-08 11:53:17\n",
            "2025-04-08 11:50:32 (UTC) - 0:07:06 - train - INFO - step: 000214 - done (%): 71.3 - loss: 0.051 - lr: 1.3e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.1 - avg_words_per_second: 131.2 - ETA: >2025-04-08 11:53:16\n",
            "2025-04-08 11:50:34 (UTC) - 0:07:07 - train - INFO - step: 000215 - done (%): 71.7 - loss: 0.064 - lr: 1.2e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.8 - avg_words_per_second: 131.4 - ETA: >2025-04-08 11:53:16\n",
            "2025-04-08 11:50:35 (UTC) - 0:07:08 - train - INFO - step: 000216 - done (%): 72.0 - loss: 0.097 - lr: 1.2e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.8 - avg_words_per_second: 131.6 - ETA: >2025-04-08 11:53:15\n",
            "2025-04-08 11:50:36 (UTC) - 0:07:10 - train - INFO - step: 000217 - done (%): 72.3 - loss: 0.040 - lr: 1.2e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.1 - avg_words_per_second: 131.8 - ETA: >2025-04-08 11:53:14\n",
            "2025-04-08 11:50:38 (UTC) - 0:07:11 - train - INFO - step: 000218 - done (%): 72.7 - loss: 0.082 - lr: 1.1e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.7 - avg_words_per_second: 132.0 - ETA: >2025-04-08 11:53:13\n",
            "2025-04-08 11:50:39 (UTC) - 0:07:12 - train - INFO - step: 000219 - done (%): 73.0 - loss: 0.014 - lr: 1.1e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.0 - avg_words_per_second: 132.1 - ETA: >2025-04-08 11:53:12\n",
            "2025-04-08 11:50:41 (UTC) - 0:07:14 - train - INFO - step: 000220 - done (%): 73.3 - loss: 0.039 - lr: 1.1e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 154.7 - avg_words_per_second: 132.2 - ETA: >2025-04-08 11:53:12\n",
            "2025-04-08 11:50:42 (UTC) - 0:07:15 - train - INFO - step: 000221 - done (%): 73.7 - loss: 0.028 - lr: 1.1e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.0 - avg_words_per_second: 132.4 - ETA: >2025-04-08 11:53:11\n",
            "2025-04-08 11:50:43 (UTC) - 0:07:17 - train - INFO - step: 000222 - done (%): 74.0 - loss: 0.087 - lr: 1.0e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 192.4 - avg_words_per_second: 132.6 - ETA: >2025-04-08 11:53:10\n",
            "2025-04-08 11:50:44 (UTC) - 0:07:18 - train - INFO - step: 000223 - done (%): 74.3 - loss: 0.132 - lr: 1.0e-05 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.5 - avg_words_per_second: 132.8 - ETA: >2025-04-08 11:53:09\n",
            "2025-04-08 11:50:46 (UTC) - 0:07:19 - train - INFO - step: 000224 - done (%): 74.7 - loss: 0.020 - lr: 9.9e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.3 - avg_words_per_second: 133.0 - ETA: >2025-04-08 11:53:09\n",
            "2025-04-08 11:50:47 (UTC) - 0:07:20 - train - INFO - step: 000225 - done (%): 75.0 - loss: 0.084 - lr: 9.7e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.9 - avg_words_per_second: 133.2 - ETA: >2025-04-08 11:53:08\n",
            "2025-04-08 11:50:48 (UTC) - 0:07:22 - train - INFO - step: 000226 - done (%): 75.3 - loss: 0.024 - lr: 9.4e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.1 - avg_words_per_second: 133.4 - ETA: >2025-04-08 11:53:07\n",
            "2025-04-08 11:50:50 (UTC) - 0:07:23 - train - INFO - step: 000227 - done (%): 75.7 - loss: 0.056 - lr: 9.2e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.1 - avg_words_per_second: 133.5 - ETA: >2025-04-08 11:53:06\n",
            "2025-04-08 11:50:51 (UTC) - 0:07:24 - train - INFO - step: 000228 - done (%): 76.0 - loss: 0.081 - lr: 9.0e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.0 - avg_words_per_second: 133.7 - ETA: >2025-04-08 11:53:05\n",
            "2025-04-08 11:50:52 (UTC) - 0:07:26 - train - INFO - step: 000229 - done (%): 76.3 - loss: 0.054 - lr: 8.7e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.6 - avg_words_per_second: 133.9 - ETA: >2025-04-08 11:53:05\n",
            "2025-04-08 11:50:53 (UTC) - 0:07:27 - train - INFO - step: 000230 - done (%): 76.7 - loss: 0.069 - lr: 8.5e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.0 - avg_words_per_second: 134.1 - ETA: >2025-04-08 11:53:04\n",
            "2025-04-08 11:50:55 (UTC) - 0:07:28 - train - INFO - step: 000231 - done (%): 77.0 - loss: 0.040 - lr: 8.3e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.4 - avg_words_per_second: 134.3 - ETA: >2025-04-08 11:53:03\n",
            "2025-04-08 11:50:56 (UTC) - 0:07:29 - train - INFO - step: 000232 - done (%): 77.3 - loss: 0.040 - lr: 8.0e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.9 - avg_words_per_second: 134.5 - ETA: >2025-04-08 11:53:02\n",
            "2025-04-08 11:50:57 (UTC) - 0:07:31 - train - INFO - step: 000233 - done (%): 77.7 - loss: 0.047 - lr: 7.8e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 196.3 - avg_words_per_second: 134.6 - ETA: >2025-04-08 11:53:02\n",
            "2025-04-08 11:50:59 (UTC) - 0:07:32 - train - INFO - step: 000234 - done (%): 78.0 - loss: 0.034 - lr: 7.6e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 155.7 - avg_words_per_second: 134.7 - ETA: >2025-04-08 11:53:01\n",
            "2025-04-08 11:51:00 (UTC) - 0:07:34 - train - INFO - step: 000235 - done (%): 78.3 - loss: 0.073 - lr: 7.4e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 196.4 - avg_words_per_second: 134.9 - ETA: >2025-04-08 11:53:01\n",
            "2025-04-08 11:51:01 (UTC) - 0:07:35 - train - INFO - step: 000236 - done (%): 78.7 - loss: 0.060 - lr: 7.2e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.9 - avg_words_per_second: 135.1 - ETA: >2025-04-08 11:53:00\n",
            "2025-04-08 11:51:03 (UTC) - 0:07:36 - train - INFO - step: 000237 - done (%): 79.0 - loss: 0.061 - lr: 6.9e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.3 - avg_words_per_second: 135.2 - ETA: >2025-04-08 11:52:59\n",
            "2025-04-08 11:51:04 (UTC) - 0:07:37 - train - INFO - step: 000238 - done (%): 79.3 - loss: 0.024 - lr: 6.7e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.8 - avg_words_per_second: 135.4 - ETA: >2025-04-08 11:52:58\n",
            "2025-04-08 11:51:05 (UTC) - 0:07:39 - train - INFO - step: 000239 - done (%): 79.7 - loss: 0.119 - lr: 6.5e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.6 - avg_words_per_second: 135.6 - ETA: >2025-04-08 11:52:58\n",
            "2025-04-08 11:51:07 (UTC) - 0:07:40 - train - INFO - step: 000240 - done (%): 80.0 - loss: 0.024 - lr: 6.3e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.5 - avg_words_per_second: 135.8 - ETA: >2025-04-08 11:52:57\n",
            "2025-04-08 11:51:08 (UTC) - 0:07:41 - train - INFO - step: 000241 - done (%): 80.3 - loss: 0.029 - lr: 6.1e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 197.1 - avg_words_per_second: 135.9 - ETA: >2025-04-08 11:52:56\n",
            "2025-04-08 11:51:09 (UTC) - 0:07:43 - train - INFO - step: 000242 - done (%): 80.7 - loss: 0.057 - lr: 5.9e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.1 - avg_words_per_second: 136.1 - ETA: >2025-04-08 11:52:56\n",
            "2025-04-08 11:51:10 (UTC) - 0:07:44 - train - INFO - step: 000243 - done (%): 81.0 - loss: 0.037 - lr: 5.7e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 197.8 - avg_words_per_second: 136.3 - ETA: >2025-04-08 11:52:55\n",
            "2025-04-08 11:51:12 (UTC) - 0:07:45 - train - INFO - step: 000244 - done (%): 81.3 - loss: 0.128 - lr: 5.5e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.8 - avg_words_per_second: 136.5 - ETA: >2025-04-08 11:52:54\n",
            "2025-04-08 11:51:13 (UTC) - 0:07:46 - train - INFO - step: 000245 - done (%): 81.7 - loss: 0.022 - lr: 5.3e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 186.2 - avg_words_per_second: 136.6 - ETA: >2025-04-08 11:52:54\n",
            "2025-04-08 11:51:14 (UTC) - 0:07:48 - train - INFO - step: 000246 - done (%): 82.0 - loss: 0.149 - lr: 5.2e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 191.8 - avg_words_per_second: 136.8 - ETA: >2025-04-08 11:52:53\n",
            "2025-04-08 11:51:16 (UTC) - 0:07:49 - train - INFO - step: 000247 - done (%): 82.3 - loss: 0.054 - lr: 5.0e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.7 - avg_words_per_second: 136.9 - ETA: >2025-04-08 11:52:52\n",
            "2025-04-08 11:51:17 (UTC) - 0:07:50 - train - INFO - step: 000248 - done (%): 82.7 - loss: 0.034 - lr: 4.8e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.7 - avg_words_per_second: 137.1 - ETA: >2025-04-08 11:52:52\n",
            "2025-04-08 11:51:19 (UTC) - 0:07:52 - train - INFO - step: 000249 - done (%): 83.0 - loss: 0.016 - lr: 4.6e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 155.2 - avg_words_per_second: 137.2 - ETA: >2025-04-08 11:52:51\n",
            "2025-04-08 11:51:20 (UTC) - 0:07:53 - train - INFO - step: 000250 - done (%): 83.3 - loss: 0.051 - lr: 4.4e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 198.9 - avg_words_per_second: 137.3 - ETA: >2025-04-08 11:52:51\n",
            "2025-04-08 11:51:21 (UTC) - 0:07:55 - train - INFO - step: 000251 - done (%): 83.7 - loss: 0.073 - lr: 4.3e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 197.8 - avg_words_per_second: 137.5 - ETA: >2025-04-08 11:52:50\n",
            "2025-04-08 11:51:22 (UTC) - 0:07:56 - train - INFO - step: 000252 - done (%): 84.0 - loss: 0.023 - lr: 4.1e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.8 - avg_words_per_second: 137.7 - ETA: >2025-04-08 11:52:50\n",
            "2025-04-08 11:51:24 (UTC) - 0:07:57 - train - INFO - step: 000253 - done (%): 84.3 - loss: 0.099 - lr: 3.9e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.6 - avg_words_per_second: 137.8 - ETA: >2025-04-08 11:52:49\n",
            "2025-04-08 11:51:25 (UTC) - 0:07:58 - train - INFO - step: 000254 - done (%): 84.7 - loss: 0.348 - lr: 3.8e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.1 - avg_words_per_second: 138.0 - ETA: >2025-04-08 11:52:48\n",
            "2025-04-08 11:51:26 (UTC) - 0:08:00 - train - INFO - step: 000255 - done (%): 85.0 - loss: 0.025 - lr: 3.6e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 196.5 - avg_words_per_second: 138.1 - ETA: >2025-04-08 11:52:48\n",
            "2025-04-08 11:51:27 (UTC) - 0:08:01 - train - INFO - step: 000256 - done (%): 85.3 - loss: 0.134 - lr: 3.5e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 192.8 - avg_words_per_second: 138.3 - ETA: >2025-04-08 11:52:47\n",
            "2025-04-08 11:51:29 (UTC) - 0:08:02 - train - INFO - step: 000257 - done (%): 85.7 - loss: 0.016 - lr: 3.3e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.2 - avg_words_per_second: 138.4 - ETA: >2025-04-08 11:52:46\n",
            "2025-04-08 11:51:30 (UTC) - 0:08:04 - train - INFO - step: 000258 - done (%): 86.0 - loss: 0.025 - lr: 3.2e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.1 - avg_words_per_second: 138.6 - ETA: >2025-04-08 11:52:46\n",
            "2025-04-08 11:51:31 (UTC) - 0:08:05 - train - INFO - step: 000259 - done (%): 86.3 - loss: 0.029 - lr: 3.0e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.2 - avg_words_per_second: 138.7 - ETA: >2025-04-08 11:52:45\n",
            "2025-04-08 11:51:33 (UTC) - 0:08:06 - train - INFO - step: 000260 - done (%): 86.7 - loss: 0.060 - lr: 2.9e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.1 - avg_words_per_second: 138.9 - ETA: >2025-04-08 11:52:45\n",
            "2025-04-08 11:51:34 (UTC) - 0:08:07 - train - INFO - step: 000261 - done (%): 87.0 - loss: 0.072 - lr: 2.7e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 192.7 - avg_words_per_second: 139.0 - ETA: >2025-04-08 11:52:44\n",
            "2025-04-08 11:51:35 (UTC) - 0:08:09 - train - INFO - step: 000262 - done (%): 87.3 - loss: 0.018 - lr: 2.6e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 196.0 - avg_words_per_second: 139.2 - ETA: >2025-04-08 11:52:43\n",
            "2025-04-08 11:51:36 (UTC) - 0:08:10 - train - INFO - step: 000263 - done (%): 87.7 - loss: 0.123 - lr: 2.5e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.2 - avg_words_per_second: 139.4 - ETA: >2025-04-08 11:52:43\n",
            "2025-04-08 11:51:38 (UTC) - 0:08:12 - train - INFO - step: 000264 - done (%): 88.0 - loss: 0.100 - lr: 2.3e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 155.1 - avg_words_per_second: 139.4 - ETA: >2025-04-08 11:52:43\n",
            "2025-04-08 11:51:39 (UTC) - 0:08:13 - train - INFO - step: 000265 - done (%): 88.3 - loss: 0.026 - lr: 2.2e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.3 - avg_words_per_second: 139.6 - ETA: >2025-04-08 11:52:42\n",
            "2025-04-08 11:51:41 (UTC) - 0:08:14 - train - INFO - step: 000266 - done (%): 88.7 - loss: 0.034 - lr: 2.1e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.8 - avg_words_per_second: 139.7 - ETA: >2025-04-08 11:52:42\n",
            "2025-04-08 11:51:42 (UTC) - 0:08:15 - train - INFO - step: 000267 - done (%): 89.0 - loss: 0.059 - lr: 2.0e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.4 - avg_words_per_second: 139.9 - ETA: >2025-04-08 11:52:41\n",
            "2025-04-08 11:51:43 (UTC) - 0:08:17 - train - INFO - step: 000268 - done (%): 89.3 - loss: 0.131 - lr: 1.8e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 196.5 - avg_words_per_second: 140.0 - ETA: >2025-04-08 11:52:40\n",
            "2025-04-08 11:51:45 (UTC) - 0:08:18 - train - INFO - step: 000269 - done (%): 89.7 - loss: 0.026 - lr: 1.7e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.7 - avg_words_per_second: 140.2 - ETA: >2025-04-08 11:52:40\n",
            "2025-04-08 11:51:46 (UTC) - 0:08:19 - train - INFO - step: 000270 - done (%): 90.0 - loss: 0.078 - lr: 1.6e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.5 - avg_words_per_second: 140.3 - ETA: >2025-04-08 11:52:39\n",
            "2025-04-08 11:51:47 (UTC) - 0:08:21 - train - INFO - step: 000271 - done (%): 90.3 - loss: 0.015 - lr: 1.5e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 196.5 - avg_words_per_second: 140.4 - ETA: >2025-04-08 11:52:39\n",
            "2025-04-08 11:51:48 (UTC) - 0:08:22 - train - INFO - step: 000272 - done (%): 90.7 - loss: 0.040 - lr: 1.4e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.6 - avg_words_per_second: 140.6 - ETA: >2025-04-08 11:52:38\n",
            "2025-04-08 11:51:50 (UTC) - 0:08:23 - train - INFO - step: 000273 - done (%): 91.0 - loss: 0.066 - lr: 1.3e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.2 - avg_words_per_second: 140.7 - ETA: >2025-04-08 11:52:38\n",
            "2025-04-08 11:51:51 (UTC) - 0:08:24 - train - INFO - step: 000274 - done (%): 91.3 - loss: 0.061 - lr: 1.2e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.1 - avg_words_per_second: 140.9 - ETA: >2025-04-08 11:52:37\n",
            "2025-04-08 11:51:52 (UTC) - 0:08:26 - train - INFO - step: 000275 - done (%): 91.7 - loss: 0.029 - lr: 1.1e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.7 - avg_words_per_second: 141.0 - ETA: >2025-04-08 11:52:37\n",
            "2025-04-08 11:51:54 (UTC) - 0:08:27 - train - INFO - step: 000276 - done (%): 92.0 - loss: 0.042 - lr: 1.0e-06 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.4 - avg_words_per_second: 141.1 - ETA: >2025-04-08 11:52:36\n",
            "2025-04-08 11:51:55 (UTC) - 0:08:28 - train - INFO - step: 000277 - done (%): 92.3 - loss: 0.148 - lr: 9.6e-07 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.7 - avg_words_per_second: 141.3 - ETA: >2025-04-08 11:52:36\n",
            "2025-04-08 11:51:56 (UTC) - 0:08:30 - train - INFO - step: 000278 - done (%): 92.7 - loss: 0.015 - lr: 8.8e-07 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.3 - avg_words_per_second: 141.4 - ETA: >2025-04-08 11:52:35\n",
            "2025-04-08 11:51:58 (UTC) - 0:08:31 - train - INFO - step: 000279 - done (%): 93.0 - loss: 0.040 - lr: 8.0e-07 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 155.4 - avg_words_per_second: 141.5 - ETA: >2025-04-08 11:52:35\n",
            "2025-04-08 11:51:59 (UTC) - 0:08:32 - train - INFO - step: 000280 - done (%): 93.3 - loss: 0.060 - lr: 7.3e-07 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.5 - avg_words_per_second: 141.6 - ETA: >2025-04-08 11:52:34\n",
            "2025-04-08 11:52:00 (UTC) - 0:08:34 - train - INFO - step: 000281 - done (%): 93.7 - loss: 0.036 - lr: 6.6e-07 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.8 - avg_words_per_second: 141.7 - ETA: >2025-04-08 11:52:34\n",
            "2025-04-08 11:52:02 (UTC) - 0:08:35 - train - INFO - step: 000282 - done (%): 94.0 - loss: 0.046 - lr: 5.9e-07 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.9 - avg_words_per_second: 141.9 - ETA: >2025-04-08 11:52:33\n",
            "2025-04-08 11:52:03 (UTC) - 0:08:36 - train - INFO - step: 000283 - done (%): 94.3 - loss: 0.026 - lr: 5.3e-07 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.3 - avg_words_per_second: 142.0 - ETA: >2025-04-08 11:52:33\n",
            "2025-04-08 11:52:04 (UTC) - 0:08:38 - train - INFO - step: 000284 - done (%): 94.7 - loss: 0.111 - lr: 4.7e-07 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 191.7 - avg_words_per_second: 142.1 - ETA: >2025-04-08 11:52:32\n",
            "2025-04-08 11:52:05 (UTC) - 0:08:39 - train - INFO - step: 000285 - done (%): 95.0 - loss: 0.019 - lr: 4.1e-07 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 197.9 - avg_words_per_second: 142.3 - ETA: >2025-04-08 11:52:32\n",
            "2025-04-08 11:52:07 (UTC) - 0:08:40 - train - INFO - step: 000286 - done (%): 95.3 - loss: 0.023 - lr: 3.6e-07 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.7 - avg_words_per_second: 142.4 - ETA: >2025-04-08 11:52:31\n",
            "2025-04-08 11:52:08 (UTC) - 0:08:41 - train - INFO - step: 000287 - done (%): 95.7 - loss: 0.018 - lr: 3.1e-07 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 197.5 - avg_words_per_second: 142.6 - ETA: >2025-04-08 11:52:31\n",
            "2025-04-08 11:52:09 (UTC) - 0:08:43 - train - INFO - step: 000288 - done (%): 96.0 - loss: 0.054 - lr: 2.6e-07 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 196.2 - avg_words_per_second: 142.7 - ETA: >2025-04-08 11:52:30\n",
            "2025-04-08 11:52:11 (UTC) - 0:08:44 - train - INFO - step: 000289 - done (%): 96.3 - loss: 0.063 - lr: 2.2e-07 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 196.6 - avg_words_per_second: 142.8 - ETA: >2025-04-08 11:52:30\n",
            "2025-04-08 11:52:12 (UTC) - 0:08:45 - train - INFO - step: 000290 - done (%): 96.7 - loss: 0.048 - lr: 1.8e-07 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 196.7 - avg_words_per_second: 143.0 - ETA: >2025-04-08 11:52:29\n",
            "2025-04-08 11:52:13 (UTC) - 0:08:47 - train - INFO - step: 000291 - done (%): 97.0 - loss: 0.111 - lr: 1.5e-07 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 192.3 - avg_words_per_second: 143.1 - ETA: >2025-04-08 11:52:29\n",
            "2025-04-08 11:52:14 (UTC) - 0:08:48 - train - INFO - step: 000292 - done (%): 97.3 - loss: 0.019 - lr: 1.2e-07 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.3 - avg_words_per_second: 143.2 - ETA: >2025-04-08 11:52:28\n",
            "2025-04-08 11:52:16 (UTC) - 0:08:49 - train - INFO - step: 000293 - done (%): 97.7 - loss: 0.042 - lr: 9.0e-08 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.5 - avg_words_per_second: 143.4 - ETA: >2025-04-08 11:52:28\n",
            "2025-04-08 11:52:17 (UTC) - 0:08:50 - train - INFO - step: 000294 - done (%): 98.0 - loss: 0.025 - lr: 6.6e-08 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.9 - avg_words_per_second: 143.5 - ETA: >2025-04-08 11:52:27\n",
            "2025-04-08 11:52:19 (UTC) - 0:08:52 - train - INFO - step: 000295 - done (%): 98.3 - loss: 0.034 - lr: 4.6e-08 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 157.2 - avg_words_per_second: 143.5 - ETA: >2025-04-08 11:52:27\n",
            "2025-04-08 11:52:20 (UTC) - 0:08:53 - train - INFO - step: 000296 - done (%): 98.7 - loss: 0.114 - lr: 2.9e-08 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 197.5 - avg_words_per_second: 143.7 - ETA: >2025-04-08 11:52:27\n",
            "2025-04-08 11:52:21 (UTC) - 0:08:55 - train - INFO - step: 000297 - done (%): 99.0 - loss: 0.030 - lr: 1.7e-08 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 195.4 - avg_words_per_second: 143.8 - ETA: >2025-04-08 11:52:26\n",
            "2025-04-08 11:52:22 (UTC) - 0:08:56 - train - INFO - step: 000298 - done (%): 99.3 - loss: 0.074 - lr: 7.5e-09 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 194.7 - avg_words_per_second: 143.9 - ETA: >2025-04-08 11:52:26\n",
            "2025-04-08 11:52:24 (UTC) - 0:08:57 - train - INFO - step: 000299 - done (%): 99.7 - loss: 0.039 - lr: 2.1e-09 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 193.7 - avg_words_per_second: 144.0 - ETA: >2025-04-08 11:52:25\n",
            "2025-04-08 11:52:25 (UTC) - 0:08:58 - eval - INFO - Start eval...\n",
            "2025-04-08 11:53:27 (UTC) - 0:10:01 - eval - INFO - Eval finished!\n",
            "2025-04-08 11:53:27 (UTC) - 0:10:01 - train - INFO - step: 000300 - eval_perplexity: 1.036 - eval_loss: 0.051 - train_loss: 0.024\n",
            "2025-04-08 11:53:27 (UTC) - 0:10:01 - train - INFO - step: 000300 - done (%): 100.0 - loss: 0.024 - lr: 2.4e-10 - peak_alloc_mem (GB): 17.1 - alloc_mem (GB): 16.2 - words_per_second: 3.9 - avg_words_per_second: 128.7 - ETA: >2025-04-08 11:53:27\n",
            "2025-04-08 11:53:27 (UTC) - 0:10:01 - checkpointing - INFO - Dumping checkpoint in tune_model/checkpoints/checkpoint_000300/consolidated using tmp name: tmp.consolidated\n",
            "2025-04-08 11:53:28 (UTC) - 0:10:02 - checkpointing - INFO - Done dumping checkpoint in tune_model/checkpoints/checkpoint_000300/consolidated for step: 300\n",
            "2025-04-08 11:53:28 (UTC) - 0:10:02 - checkpointing - INFO - Done deleting checkpoints \n",
            "2025-04-08 11:53:28 (UTC) - 0:10:02 - checkpointing - INFO - Done!\n",
            "2025-04-08 11:53:28 (UTC) - 0:10:02 - train - INFO - done!\n",
            "2025-04-08 11:53:28 (UTC) - 0:10:02 - utils - INFO - Closing: eval_logger\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m uploading output.log 76.2KB/76.2KB (0.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m uploading wandb-summary.json 465B/465B (0.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m uploading config.yaml 1.9KB/1.9KB (0.3s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m uploading output.log 76.2KB/76.2KB (0.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m uploading wandb-summary.json 465B/465B (0.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m uploading config.yaml 1.9KB/1.9KB (0.3s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m uploading output.log 76.2KB/76.2KB (0.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m uploading wandb-summary.json 465B/465B (0.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m uploading config.yaml 1.9KB/1.9KB (0.3s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m uploading output.log 76.2KB/76.2KB (0.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m uploading wandb-summary.json 465B/465B (0.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m uploading config.yaml 1.9KB/1.9KB (0.3s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m uploading output.log 76.2KB/76.2KB (0.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m uploading wandb-summary.json 465B/465B (0.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m uploading config.yaml 1.9KB/1.9KB (0.3s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m uploading output.log 76.2KB/76.2KB (0.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m uploading wandb-summary.json 465B/465B (0.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m uploading config.yaml 1.9KB/1.9KB (0.3s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣟\u001b[0m uploading output.log 76.2KB/76.2KB (0.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣟\u001b[0m uploading wandb-summary.json 465B/465B (0.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣟\u001b[0m uploading config.yaml 1.9KB/1.9KB (0.3s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⡿\u001b[0m uploading output.log 76.2KB/76.2KB (0.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⡿\u001b[0m uploading wandb-summary.json 465B/465B (0.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⡿\u001b[0m uploading config.yaml 1.9KB/1.9KB (0.3s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m uploading output.log 76.2KB/76.2KB (0.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m uploading wandb-summary.json 465B/465B (0.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m uploading config.yaml 1.9KB/1.9KB (0.3s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m uploading history steps 298-299, summary, console lines 332-340 (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m uploading history steps 298-299, summary, console lines 332-340 (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           eval/eval_loss █▃▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/perplexity █▃▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/train_loss █▄▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/allocated_mem ▁███████████████████████████████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/avg_wps ▆▆▇▇▇████████▁▁▂▂▂▂▂▃▃▃▃▄▁▁▁▁▂▂▂▂▂▂▂▂▂▃▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train/eta_in_seconds █▆▆▅▅▅▅▅▄▄▄▄▄▅▄▄▄▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/loss █▂▂▂▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/lr █████▇▇▆▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/peak_allocated_mem ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/percent_done ▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                train/wps ██████████████████████████▇████████████▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           eval/eval_loss 0.05096\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/perplexity 1.03596\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/train_loss 0.02374\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/allocated_mem 16.15987\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/avg_wps 128.72271\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train/eta_in_seconds 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/loss 0.02374\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/lr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/peak_allocated_mem 17.05618\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/percent_done 100\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                train/wps 3.92591\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mtune_model\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/211501016-rec/mistral%20finetune/runs/0o4gzp8v\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/211501016-rec/mistral%20finetune\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1mtune_model/wandb/run-20250408_114335-0o4gzp8v/logs\u001b[0m\n",
            "2025-04-08 11:53:30 (UTC) - 0:10:04 - utils - INFO - Closed: eval_logger\n",
            "2025-04-08 11:53:30 (UTC) - 0:10:04 - utils - INFO - Closing: metrics_logger\n",
            "2025-04-08 11:53:30 (UTC) - 0:10:04 - utils - INFO - Closed: metrics_logger\n",
            "2025-04-08 11:53:30 (UTC) - 0:10:04 - train - INFO - Closed everything!\n",
            "[rank0]:[W408 11:53:31.502196262 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
          ]
        }
      ],
      "source": [
        "# start training\n",
        "\n",
        "!torchrun --nproc-per-node 1 -m train example.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruJ29JFn98zE"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BWNGKt9-Kxz",
        "outputId": "7f41a625-528a-45aa-839f-7e52d4bc93a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mistral_inference in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: fire>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from mistral_inference) (0.7.0)\n",
            "Requirement already satisfied: mistral_common>=1.5.4 in /usr/local/lib/python3.11/dist-packages (from mistral_inference) (1.5.4)\n",
            "Requirement already satisfied: pillow>=10.3.0 in /usr/local/lib/python3.11/dist-packages (from mistral_inference) (11.1.0)\n",
            "Requirement already satisfied: safetensors>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mistral_inference) (0.5.3)\n",
            "Requirement already satisfied: simple-parsing>=0.1.5 in /usr/local/lib/python3.11/dist-packages (from mistral_inference) (0.1.7)\n",
            "Requirement already satisfied: xformers>=0.0.24 in /usr/local/lib/python3.11/dist-packages (from mistral_inference) (0.0.29.post3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire>=0.6.0->mistral_inference) (3.0.1)\n",
            "Requirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.11/dist-packages (from mistral_common>=1.5.4->mistral_inference) (4.23.0)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.11/dist-packages (from mistral_common>=1.5.4->mistral_inference) (2.2.4)\n",
            "Requirement already satisfied: pydantic<3.0,>=2.7 in /usr/local/lib/python3.11/dist-packages (from mistral_common>=1.5.4->mistral_inference) (2.11.2)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from mistral_common>=1.5.4->mistral_inference) (2.32.3)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from mistral_common>=1.5.4->mistral_inference) (0.2.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from mistral_common>=1.5.4->mistral_inference) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from mistral_common>=1.5.4->mistral_inference) (4.13.1)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.11/dist-packages (from simple-parsing>=0.1.5->mistral_inference) (0.16)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from xformers>=0.0.24->mistral_inference) (2.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers>=0.0.24->mistral_inference) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers>=0.0.24->mistral_inference) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers>=0.0.24->mistral_inference) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers>=0.0.24->mistral_inference) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers>=0.0.24->mistral_inference) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers>=0.0.24->mistral_inference) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers>=0.0.24->mistral_inference) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers>=0.0.24->mistral_inference) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers>=0.0.24->mistral_inference) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers>=0.0.24->mistral_inference) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers>=0.0.24->mistral_inference) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers>=0.0.24->mistral_inference) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers>=0.0.24->mistral_inference) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers>=0.0.24->mistral_inference) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers>=0.0.24->mistral_inference) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers>=0.0.24->mistral_inference) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers>=0.0.24->mistral_inference) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers>=0.0.24->mistral_inference) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers>=0.0.24->mistral_inference) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->xformers>=0.0.24->mistral_inference) (1.3.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.5.4->mistral_inference) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.5.4->mistral_inference) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.5.4->mistral_inference) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.5.4->mistral_inference) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=2.7->mistral_common>=1.5.4->mistral_inference) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=2.7->mistral_common>=1.5.4->mistral_inference) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=2.7->mistral_common>=1.5.4->mistral_inference) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->mistral_common>=1.5.4->mistral_inference) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->mistral_common>=1.5.4->mistral_inference) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->mistral_common>=1.5.4->mistral_inference) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->mistral_common>=1.5.4->mistral_inference) (2025.1.31)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.7.0->mistral_common>=1.5.4->mistral_inference) (2024.11.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->xformers>=0.0.24->mistral_inference) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install mistral_inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from mistral_inference.transformer import Transformer\n",
        "\n",
        "model = Transformer.from_folder(\"/content/mistral_models\", dtype=torch.bfloat16)"
      ],
      "metadata": {
        "id": "jMfO1CasQ-2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "F-xLs2Ot9-il",
        "outputId": "33b43dbf-95f8-4b41-b935-6b6e5c296e1d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 136.88 MiB is free. Process 226004 has 39.41 GiB memory in use. Of the allocated memory 38.80 GiB is allocated by PyTorch, and 124.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-bc91fb64fe31>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMistralTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/mistral_models/tokenizer.model.v3\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# change to extracted tokenizer file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/mistral_models\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# change to extracted model dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lora\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/mistral-finetune/tune_model/checkpoints/checkpoint_000300/consolidated/lora.safetensors\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mistral_inference/transformer.py\u001b[0m in \u001b[0;36mfrom_folder\u001b[0;34m(folder, max_batch_size, num_pipeline_ranks, device, dtype, softmax_fp32)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     )\n\u001b[0;32m-> 1329\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 136.88 MiB is free. Process 226004 has 39.41 GiB memory in use. Of the allocated memory 38.80 GiB is allocated by PyTorch, and 124.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "from mistral_inference.transformer import Transformer\n",
        "from mistral_inference.generate import generate\n",
        "\n",
        "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
        "from mistral_common.protocol.instruct.messages import UserMessage\n",
        "from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
        "\n",
        "\n",
        "tokenizer = MistralTokenizer.from_file(\"/content/mistral_models/tokenizer.model.v3\")  # change to extracted tokenizer file\n",
        "model = Transformer.from_folder(\"/content/mistral_models\")  # change to extracted model dir\n",
        "model.load_lora(\"/content/mistral-finetune/tune_model/checkpoints/checkpoint_000300/consolidated/lora.safetensors\")\n",
        "\n",
        "completion_request = ChatCompletionRequest(messages=[UserMessage(content=\"Explain Machine Learning to me in a nutshell.\")])\n",
        "\n",
        "tokens = tokenizer.encode_chat_completion(completion_request).tokens\n",
        "\n",
        "out_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\n",
        "result = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"tune_model.zip\")"
      ],
      "metadata": {
        "id": "1BU5FnGRUY19",
        "outputId": "945752df-4811-40d5-87b4-8371f0339a77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6be0d2b5-9fd1-48cb-b662-6a97e00f47c1\", \"tune_model.zip\", 791736360)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from mistral_inference.transformer import Transformer\n",
        "\n",
        "model = Transformer.from_folder(\"/content/mistral_models\", dtype=torch.bfloat16)"
      ],
      "metadata": {
        "id": "CXh7B_hfVTCA",
        "outputId": "16fd82a6-19e7-474b-ac74-8d98abe8b160",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 136.88 MiB is free. Process 226004 has 39.41 GiB memory in use. Of the allocated memory 38.80 GiB is allocated by PyTorch, and 124.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-8bc91812c050>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmistral_inference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/mistral_models\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mistral_inference/transformer.py\u001b[0m in \u001b[0;36mfrom_folder\u001b[0;34m(folder, max_batch_size, num_pipeline_ranks, device, dtype, softmax_fp32)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     )\n\u001b[0;32m-> 1329\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 136.88 MiB is free. Process 226004 has 39.41 GiB memory in use. Of the allocated memory 38.80 GiB is allocated by PyTorch, and 124.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "discounts = {}  # Dictionary to track user discounts\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"You are a smart bargaining assistant.\n",
        "You help users negotiate the best price while maintaining a fair and engaging conversation.\n",
        "Never go below the minimum allowed discount of 10%, and cap maximum discounts at 50%.\n",
        "Encourage the user to make a deal, but also try to upsell additional products.\"\"\"\n",
        "\n",
        "def generate_response(user_id, input_text):\n",
        "    if user_id not in discounts:\n",
        "        discounts[user_id] = 10  # Start with a 10% discount\n",
        "\n",
        "    increase = random.choice([5, 10])  # Randomly increase discount\n",
        "    discounts[user_id] += increase\n",
        "    if discounts[user_id] > 50:  # Cap at 50%\n",
        "        discounts[user_id] = 50\n",
        "\n",
        "    prompt = f\"{SYSTEM_PROMPT}\\nUser: {input_text}\\nAssistant:\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    output = model.generate(**inputs, max_length=150, temperature=0.7, top_p=0.9)\n",
        "\n",
        "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return {\"response\": response, \"discount\": discounts[user_id]}\n"
      ],
      "metadata": {
        "id": "QIScR1sMWKu7"
      },
      "execution_count": 61,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4f3682060fdf4a049ac322b66feab150": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_451d2211a82c48efae1da4ecf5b3fceb"
          }
        },
        "b61bc4cd66144d6e909f0a167f590940": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f3489577d554a059455f4ad4b7e37d9",
            "placeholder": "​",
            "style": "IPY_MODEL_e98668ed12b74880ae0a9e10ee890776",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "b4f8f0547d4746a482a2f747ea9a7402": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_5f5e36f314604560a0579b8ffe82aac6",
            "placeholder": "​",
            "style": "IPY_MODEL_46917fde00c2434fbee53512a211b9d0",
            "value": ""
          }
        },
        "ab1620f140414c73842471cb782cc0fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_4d494354113247c0b9e7360b801aafe0",
            "style": "IPY_MODEL_44137a5f09bb48acb8af27774ad007f2",
            "value": true
          }
        },
        "53cc287bd1024a618d22dbf3ba681a26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_8ff263bc2e154014a800a5cf9489d14c",
            "style": "IPY_MODEL_bec5191b2fb149369a5801bc3f720910",
            "tooltip": ""
          }
        },
        "771b761bc9534baf95068bcb70b3322d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddf54d61079e43379d73fd364f644a72",
            "placeholder": "​",
            "style": "IPY_MODEL_e5879853dc564d788985177906d464d3",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "451d2211a82c48efae1da4ecf5b3fceb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "4f3489577d554a059455f4ad4b7e37d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e98668ed12b74880ae0a9e10ee890776": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f5e36f314604560a0579b8ffe82aac6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46917fde00c2434fbee53512a211b9d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d494354113247c0b9e7360b801aafe0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44137a5f09bb48acb8af27774ad007f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ff263bc2e154014a800a5cf9489d14c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bec5191b2fb149369a5801bc3f720910": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "ddf54d61079e43379d73fd364f644a72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5879853dc564d788985177906d464d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "652e682ec6b34c88a33de00fcf3fc4a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56d92d0589fc4490a9c7fd32d2541fcb",
            "placeholder": "​",
            "style": "IPY_MODEL_a8a759511ec04c21a7e97bb7dd88cb68",
            "value": "Connecting..."
          }
        },
        "56d92d0589fc4490a9c7fd32d2541fcb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8a759511ec04c21a7e97bb7dd88cb68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1312c0c0e45d48489a429768fcf640e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2f55ec47a5ad4974a61e1005f5131002",
              "IPY_MODEL_9b02b8b3224c4260ba831121c9ecbb4f",
              "IPY_MODEL_fe01c190021046be819dfd5f1c47804c"
            ],
            "layout": "IPY_MODEL_285d987545ad4b1bbe7f6ce5d64b1ff1"
          }
        },
        "2f55ec47a5ad4974a61e1005f5131002": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_997432b383ac499bb54de0f132636ca1",
            "placeholder": "​",
            "style": "IPY_MODEL_48d400da41cd413aa31e7d613da13d38",
            "value": "Fetching 3 files: 100%"
          }
        },
        "9b02b8b3224c4260ba831121c9ecbb4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_198a4b423dd34db192863373f08cc7ea",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_22aabfbfdf09420bbf42a7f4c56f8af5",
            "value": 3
          }
        },
        "fe01c190021046be819dfd5f1c47804c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b95b03f2c4d84f6d9214decc079efe84",
            "placeholder": "​",
            "style": "IPY_MODEL_e5a59ee8a6e64fc3a133e5a925073a08",
            "value": " 3/3 [00:42&lt;00:00, 42.02s/it]"
          }
        },
        "285d987545ad4b1bbe7f6ce5d64b1ff1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "997432b383ac499bb54de0f132636ca1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48d400da41cd413aa31e7d613da13d38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "198a4b423dd34db192863373f08cc7ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22aabfbfdf09420bbf42a7f4c56f8af5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b95b03f2c4d84f6d9214decc079efe84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5a59ee8a6e64fc3a133e5a925073a08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a73419dcb04e46dcb39dfa2a86dafd44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_644101346cce4924b39e42c7bd85040a",
              "IPY_MODEL_0a431b655d4c43d59842464434d60a35",
              "IPY_MODEL_4639f94c4aa8407791b99452e9da21d5"
            ],
            "layout": "IPY_MODEL_bf0237be9f9e49e4a28d603828ba3a65"
          }
        },
        "644101346cce4924b39e42c7bd85040a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdb936c7af1e44f6b7143509ff560136",
            "placeholder": "​",
            "style": "IPY_MODEL_d2076b1482d648939b7de87ca60d4e5d",
            "value": "consolidated.safetensors: 100%"
          }
        },
        "0a431b655d4c43d59842464434d60a35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d7f94a12f404495b639424b6924816c",
            "max": 14496078512,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_295d23f05afb4e7ba9f3675f4f717eeb",
            "value": 14496078512
          }
        },
        "4639f94c4aa8407791b99452e9da21d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8382047a0c2a4b2ab7e657f2665380c5",
            "placeholder": "​",
            "style": "IPY_MODEL_e5e9b4976950424f9765bd3cad6bc35f",
            "value": " 14.5G/14.5G [00:41&lt;00:00, 163MB/s]"
          }
        },
        "bf0237be9f9e49e4a28d603828ba3a65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdb936c7af1e44f6b7143509ff560136": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2076b1482d648939b7de87ca60d4e5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d7f94a12f404495b639424b6924816c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "295d23f05afb4e7ba9f3675f4f717eeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8382047a0c2a4b2ab7e657f2665380c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5e9b4976950424f9765bd3cad6bc35f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "226d2b86525945548dd6deebbcbc08ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9cc4feebd41a429e94a0c486c4b9aa64",
              "IPY_MODEL_505bb26171f345e2aa0d2057813edd52",
              "IPY_MODEL_a4bd417593934a7f950abf7f56b3c710"
            ],
            "layout": "IPY_MODEL_c1747750c0834bdfa20addc4ee7c26ea"
          }
        },
        "9cc4feebd41a429e94a0c486c4b9aa64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87cf4478879d4693addad51719000417",
            "placeholder": "​",
            "style": "IPY_MODEL_3a6c6433056646d491dedc4381e1d79f",
            "value": "params.json: 100%"
          }
        },
        "505bb26171f345e2aa0d2057813edd52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aad1ceab8e814dc6a95d3bba99942e12",
            "max": 202,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_70b53663a89942b38bf3b165d0473dce",
            "value": 202
          }
        },
        "a4bd417593934a7f950abf7f56b3c710": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de5e686b8f164065a6d6365e2f59498c",
            "placeholder": "​",
            "style": "IPY_MODEL_6a9825f96c2842e48aa2a06adbdd28a4",
            "value": " 202/202 [00:00&lt;00:00, 12.0kB/s]"
          }
        },
        "c1747750c0834bdfa20addc4ee7c26ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87cf4478879d4693addad51719000417": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a6c6433056646d491dedc4381e1d79f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aad1ceab8e814dc6a95d3bba99942e12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70b53663a89942b38bf3b165d0473dce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "de5e686b8f164065a6d6365e2f59498c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a9825f96c2842e48aa2a06adbdd28a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99333a3f14324ca0bb68a87cdde4eae9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f928701607494b09a98c3cc21257674d",
              "IPY_MODEL_d4b2fb6088e042b4970be5f3eae9206b",
              "IPY_MODEL_9405a81f4bca4351a735d75db373e034"
            ],
            "layout": "IPY_MODEL_99466760316643d7a2357fdb2cdd28de"
          }
        },
        "f928701607494b09a98c3cc21257674d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a34e83133f4b41f78736d1158c4a9f2f",
            "placeholder": "​",
            "style": "IPY_MODEL_073c11995613468bbc2b2c02c1b90822",
            "value": "tokenizer.model.v3: 100%"
          }
        },
        "d4b2fb6088e042b4970be5f3eae9206b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24bc335a0da94c31995fd1a15c056fcf",
            "max": 587404,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3ee56ade9cbe4b6ca25d234bbb2b6651",
            "value": 587404
          }
        },
        "9405a81f4bca4351a735d75db373e034": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b27e09fcaf9648908833b7cec447b6bd",
            "placeholder": "​",
            "style": "IPY_MODEL_b8db052e4316444a9c2c73d0b2190dc0",
            "value": " 587k/587k [00:00&lt;00:00, 18.2MB/s]"
          }
        },
        "99466760316643d7a2357fdb2cdd28de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a34e83133f4b41f78736d1158c4a9f2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "073c11995613468bbc2b2c02c1b90822": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24bc335a0da94c31995fd1a15c056fcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ee56ade9cbe4b6ca25d234bbb2b6651": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b27e09fcaf9648908833b7cec447b6bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8db052e4316444a9c2c73d0b2190dc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}